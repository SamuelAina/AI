{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "## Machine Translation Project\n",
    "In this notebook, sections that end with **'(IMPLEMENTATION)'** in the header indicate that the following blocks of code will require additional functionality which you must provide. Please be sure to read the instructions carefully!\n",
    "\n",
    "## Introduction\n",
    "In this notebook, you will build a deep neural network that functions as part of an end-to-end machine translation pipeline. Your completed pipeline will accept English text as input and return the French translation.\n",
    "\n",
    "- **Preprocess** - You'll convert text to sequence of integers.\n",
    "- **Models** Create models which accepts a sequence of integers as input and returns a probability distribution over possible translations. After learning about the basic types of neural networks that are often used for machine translation, you will engage in your own investigations, to design your own model!\n",
    "- **Prediction** Run the model on English text.\n",
    "\n",
    "## Dataset\n",
    "We begin by investigating the dataset that will be used to train and evaluate your pipeline.  The most common datasets used for machine translation are from [WMT](http://www.statmt.org/).  However, that will take a long time to train a neural network on.  We'll be using a dataset we created for this project that contains a small vocabulary.  You'll be able to train your model in a reasonable time with this dataset.\n",
    "### Load Data\n",
    "The data is located in `data/small_vocab_en` and `data/small_vocab_fr`. The `small_vocab_en` file contains English sentences with their French translations in the `small_vocab_fr` file. Load the English and French data from these files from running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "import helper\n",
    "\n",
    "\n",
    "# Load English data\n",
    "english_sentences = helper.load_data('data/small_vocab_en')\n",
    "# Load French data\n",
    "french_sentences = helper.load_data('data/small_vocab_fr')\n",
    "\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Files\n",
    "Each line in `small_vocab_en` contains an English sentence with the respective translation in each line of `small_vocab_fr`.  View the first two lines from each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en Line 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "small_vocab_fr Line 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "small_vocab_en Line 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "small_vocab_fr Line 2:  les Ã©tats-unis est gÃ©nÃ©ralement froid en juillet , et il gÃ¨le habituellement en novembre .\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "From looking at the sentences, you can see they have been preprocessed already.  The puncuations have been delimited using spaces. All the text have been converted to lowercase.  This should save you some time, but the text requires more preprocessing.\n",
    "### Vocabulary\n",
    "The complexity of the problem is determined by the complexity of the vocabulary.  A more complex vocabulary is a more complex problem.  Let's look at the complexity of the dataset we'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823250 English words.\n",
      "227 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
      "\n",
      "1961295 French words.\n",
      "355 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "\n",
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For comparison, _Alice's Adventures in Wonderland_ contains 2,766 unique words of a total of 15,500 words.\n",
    "## Preprocess\n",
    "For this project, you won't use text data as input to your model. Instead, you'll convert the text into sequences of integers using the following preprocess methods:\n",
    "1. Tokenize the words into ids\n",
    "2. Add padding to make all the sequences the same length.\n",
    "\n",
    "Time to start preprocessing the data...\n",
    "### Tokenize (IMPLEMENTATION)\n",
    "For a neural network to predict on text data, it first has to be turned into data it can understand. Text data like \"dog\" is a sequence of ASCII character encodings.  Since a neural network is a series of multiplication and addition operations, the input data needs to be number(s).\n",
    "\n",
    "We can turn each character into a number or each word into a number.  These are called character and word ids, respectively.  Character ids are used for character level models that generate text predictions for each character.  A word level model uses word ids that generate text predictions for each word.  Word level models tend to learn better, since they are lower in complexity, so we'll use those.\n",
    "\n",
    "Turn each sentence into a sequence of words ids using Keras's [`Tokenizer`](https://keras.io/preprocessing/text/#tokenizer) function. Use this function to tokenize `english_sentences` and `french_sentences` in the cell below.\n",
    "\n",
    "Running the cell will run `tokenize` on sample data and show output for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 19, 'quick': 2, 'over': 7, 'brown': 4, 'sentence': 21, 'study': 13, 'the': 1, 'this': 18, 'prize': 17, 'fox': 5, 'by': 10, 'lexicography': 15, 'my': 12, 'of': 14, 'jove': 11, 'short': 20, 'lazy': 8, 'a': 3, 'dog': 9, 'won': 16, 'jumps': 6}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "import project_tests as tests\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    # TODO: Implement -- Done\n",
    "    \"\"\"\n",
    "    keras.preprocessing.text.Tokenizer transforms a list of num_samples sequences (lists of scalars) \n",
    "    into a 2D Numpy array of shape  (num_samples, num_timesteps).\n",
    "    keras.preprocessing.text.Tokenizer(num_words=None,\n",
    "                                   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                   lower=True,\n",
    "                                   split=\" \",\n",
    "                                   char_level=False)\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer(num_words=355)\n",
    "    \n",
    "    #fit_on_texts method vectorizes text, x\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    \n",
    "    #return \n",
    "    #1- an array/list of integers for each tokenized sentence in x, \n",
    "    #2- a value-key dictionary for each tokenized sentence in x\n",
    "    return tokenizer.texts_to_sequences(x), tokenizer\n",
    "\n",
    "\n",
    "tests.test_tokenize(tokenize)\n",
    "\n",
    "# Tokenize Example output\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Padding (IMPLEMENTATION)\n",
    "When batching the sequence of word ids together, each sequence needs to be the same length.  Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length.\n",
    "\n",
    "Make sure all the English sequences have the same length and all the French sequences have the same length by adding padding to the **end** of each sequence using Keras's [`pad_sequences`](https://keras.io/preprocessing/sequence/#pad_sequences) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [1 2 4 5 6 7 1 8 9]\n",
      "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
      "Sequence 2 in x\n",
      "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
      "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
      "Sequence 3 in x\n",
      "  Input:  [18 19  3 20 21]\n",
      "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    # TODO: Implement -- Done\n",
    "    \"\"\"\n",
    "    pad_sequences method transforms a list of num_samples sequences (lists of scalars) into a \n",
    "    2D Numpy array of shape  (num_samples, num_timesteps).\n",
    "    \n",
    "    keras.preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype='int32',\n",
    "    padding='pre', truncating='pre', value=0.)\n",
    "    \n",
    "    truncating = remove values from sequences larger than maxlen either in the \n",
    "    beginning (pre) or in the end (post) of the sequence.\n",
    "    \n",
    "    value = value (e.g. 0) to use for padding the sequences to the desired length\n",
    "    \"\"\"\n",
    "    return pad_sequences(x, maxlen=length, padding='post', value=0.)\n",
    "tests.test_pad(pad)\n",
    "\n",
    "# Pad Tokenized output\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Preprocess Pipeline\n",
    "Your focus for this project is to build neural network architecture, so we won't ask you to create a preprocess pipeline.  Instead, we've provided you with the implementation of the `preprocess` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(english_sentences, french_sentences)\n",
    "\n",
    "print('Data Preprocessed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Models\n",
    "In this section, you will experiment with various neural network architectures.\n",
    "You will begin by training four relatively simple architectures.\n",
    "- Model 1 is a simple RNN\n",
    "- Model 2 is a RNN with Embedding\n",
    "- Model 3 is a Bidirectional RNN\n",
    "- Model 4 is an optional Encoder-Decoder RNN\n",
    "\n",
    "After experimenting with the four simple architectures, you will construct a deeper architecture that is designed to outperform all four models.\n",
    "### Ids Back to Text\n",
    "The neural network will be translating the input to words ids, which isn't the final form we want.  We want the French translation.  The function `logits_to_text` will bridge the gab between the logits from the neural network to the French translation.  You'll be using this function to better understand the output of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model 1: RNN (IMPLEMENTATION)\n",
    "![RNN](images/rnn.png)\n",
    "A basic RNN model is a good baseline for sequence data.  In this model, you'll build a RNN that translates English to French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import GRU, Input, Dense, TimeDistributed,Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "import matplotlib.pyplot as plt     #need this in order to do my vsualizations\n",
    "from keras.callbacks import History #need this in order to do my vsualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/200\n",
      "6s - loss: 3.3457 - acc: 0.3727 - val_loss: 2.6745 - val_acc: 0.4173\n",
      "Epoch 2/200\n",
      "5s - loss: 2.5596 - acc: 0.4539 - val_loss: 2.4293 - val_acc: 0.4813\n",
      "Epoch 3/200\n",
      "5s - loss: 2.2918 - acc: 0.4911 - val_loss: 2.1785 - val_acc: 0.4971\n",
      "Epoch 4/200\n",
      "5s - loss: 2.1057 - acc: 0.5050 - val_loss: 2.0344 - val_acc: 0.5201\n",
      "Epoch 5/200\n",
      "5s - loss: 1.9589 - acc: 0.5375 - val_loss: 1.8701 - val_acc: 0.5616\n",
      "Epoch 6/200\n",
      "5s - loss: 1.7986 - acc: 0.5725 - val_loss: 1.7387 - val_acc: 0.5805\n",
      "Epoch 7/200\n",
      "5s - loss: 1.6986 - acc: 0.5821 - val_loss: 1.6610 - val_acc: 0.5919\n",
      "Epoch 8/200\n",
      "5s - loss: 1.6297 - acc: 0.5915 - val_loss: 1.6007 - val_acc: 0.5946\n",
      "Epoch 9/200\n",
      "5s - loss: 1.5756 - acc: 0.5994 - val_loss: 1.5543 - val_acc: 0.6023\n",
      "Epoch 10/200\n",
      "5s - loss: 1.5395 - acc: 0.6051 - val_loss: 1.5247 - val_acc: 0.6102\n",
      "Epoch 11/200\n",
      "5s - loss: 1.5127 - acc: 0.6095 - val_loss: 1.5010 - val_acc: 0.6102\n",
      "Epoch 12/200\n",
      "5s - loss: 1.4889 - acc: 0.6145 - val_loss: 1.4782 - val_acc: 0.6181\n",
      "Epoch 13/200\n",
      "5s - loss: 1.4685 - acc: 0.6193 - val_loss: 1.4600 - val_acc: 0.6190\n",
      "Epoch 14/200\n",
      "5s - loss: 1.4508 - acc: 0.6212 - val_loss: 1.4425 - val_acc: 0.6279\n",
      "Epoch 15/200\n",
      "5s - loss: 1.4363 - acc: 0.6217 - val_loss: 1.4296 - val_acc: 0.6215\n",
      "Epoch 16/200\n",
      "5s - loss: 1.4242 - acc: 0.6224 - val_loss: 1.4193 - val_acc: 0.6212\n",
      "Epoch 17/200\n",
      "5s - loss: 1.4137 - acc: 0.6247 - val_loss: 1.4081 - val_acc: 0.6261\n",
      "Epoch 18/200\n",
      "5s - loss: 1.4036 - acc: 0.6256 - val_loss: 1.4005 - val_acc: 0.6229\n",
      "Epoch 19/200\n",
      "5s - loss: 1.3947 - acc: 0.6258 - val_loss: 1.3885 - val_acc: 0.6292\n",
      "Epoch 20/200\n",
      "5s - loss: 1.3845 - acc: 0.6269 - val_loss: 1.3807 - val_acc: 0.6222\n",
      "Epoch 21/200\n",
      "5s - loss: 1.3751 - acc: 0.6268 - val_loss: 1.3706 - val_acc: 0.6264\n",
      "Epoch 22/200\n",
      "5s - loss: 1.3656 - acc: 0.6270 - val_loss: 1.3613 - val_acc: 0.6274\n",
      "Epoch 23/200\n",
      "5s - loss: 1.3562 - acc: 0.6286 - val_loss: 1.3531 - val_acc: 0.6284\n",
      "Epoch 24/200\n",
      "5s - loss: 1.3478 - acc: 0.6288 - val_loss: 1.3444 - val_acc: 0.6252\n",
      "Epoch 25/200\n",
      "5s - loss: 1.3412 - acc: 0.6294 - val_loss: 1.3367 - val_acc: 0.6299\n",
      "Epoch 26/200\n",
      "5s - loss: 1.3395 - acc: 0.6300 - val_loss: 1.3286 - val_acc: 0.6291\n",
      "Epoch 27/200\n",
      "5s - loss: 1.3256 - acc: 0.6331 - val_loss: 1.3205 - val_acc: 0.6328\n",
      "Epoch 28/200\n",
      "5s - loss: 1.3183 - acc: 0.6349 - val_loss: 1.3183 - val_acc: 0.6335\n",
      "Epoch 29/200\n",
      "5s - loss: 1.3111 - acc: 0.6363 - val_loss: 1.3068 - val_acc: 0.6376\n",
      "Epoch 30/200\n",
      "5s - loss: 1.3276 - acc: 0.6344 - val_loss: 1.3234 - val_acc: 0.6378\n",
      "Epoch 31/200\n",
      "5s - loss: 1.3047 - acc: 0.6392 - val_loss: 1.2952 - val_acc: 0.6390\n",
      "Epoch 32/200\n",
      "5s - loss: 1.2931 - acc: 0.6416 - val_loss: 1.2879 - val_acc: 0.6434\n",
      "Epoch 33/200\n",
      "5s - loss: 1.2866 - acc: 0.6430 - val_loss: 1.2841 - val_acc: 0.6404\n",
      "Epoch 34/200\n",
      "5s - loss: 1.2813 - acc: 0.6453 - val_loss: 1.2811 - val_acc: 0.6448\n",
      "Epoch 35/200\n",
      "5s - loss: 1.2776 - acc: 0.6464 - val_loss: 1.2706 - val_acc: 0.6432\n",
      "Epoch 36/200\n",
      "5s - loss: 1.2715 - acc: 0.6486 - val_loss: 1.2671 - val_acc: 0.6551\n",
      "Epoch 37/200\n",
      "5s - loss: 1.2652 - acc: 0.6513 - val_loss: 1.2624 - val_acc: 0.6446\n",
      "Epoch 38/200\n",
      "5s - loss: 1.2603 - acc: 0.6521 - val_loss: 1.2575 - val_acc: 0.6520\n",
      "Epoch 39/200\n",
      "5s - loss: 1.2561 - acc: 0.6535 - val_loss: 1.2521 - val_acc: 0.6566\n",
      "Epoch 40/200\n",
      "5s - loss: 1.2506 - acc: 0.6539 - val_loss: 1.2503 - val_acc: 0.6545\n",
      "Epoch 41/200\n",
      "5s - loss: 1.2467 - acc: 0.6549 - val_loss: 1.2468 - val_acc: 0.6537\n",
      "Epoch 42/200\n",
      "5s - loss: 1.2409 - acc: 0.6560 - val_loss: 1.2366 - val_acc: 0.6554\n",
      "Epoch 43/200\n",
      "5s - loss: 1.2354 - acc: 0.6573 - val_loss: 1.2306 - val_acc: 0.6593\n",
      "Epoch 44/200\n",
      "5s - loss: 1.2852 - acc: 0.6498 - val_loss: 1.2593 - val_acc: 0.6583\n",
      "Epoch 45/200\n",
      "5s - loss: 1.2509 - acc: 0.6560 - val_loss: 1.2446 - val_acc: 0.6583\n",
      "Epoch 46/200\n",
      "5s - loss: 1.2429 - acc: 0.6588 - val_loss: 1.2382 - val_acc: 0.6647\n",
      "Epoch 47/200\n",
      "5s - loss: 1.2373 - acc: 0.6605 - val_loss: 1.2344 - val_acc: 0.6648\n",
      "Epoch 48/200\n",
      "5s - loss: 1.2334 - acc: 0.6627 - val_loss: 1.2297 - val_acc: 0.6640\n",
      "Epoch 49/200\n",
      "5s - loss: 1.2286 - acc: 0.6636 - val_loss: 1.2260 - val_acc: 0.6644\n",
      "Epoch 50/200\n",
      "5s - loss: 1.2251 - acc: 0.6648 - val_loss: 1.2217 - val_acc: 0.6643\n",
      "Epoch 51/200\n",
      "5s - loss: 1.2216 - acc: 0.6659 - val_loss: 1.2185 - val_acc: 0.6654\n",
      "Epoch 52/200\n",
      "5s - loss: 1.2188 - acc: 0.6664 - val_loss: 1.2153 - val_acc: 0.6680\n",
      "Epoch 53/200\n",
      "5s - loss: 1.2159 - acc: 0.6671 - val_loss: 1.2134 - val_acc: 0.6667\n",
      "Epoch 54/200\n",
      "5s - loss: 1.2129 - acc: 0.6684 - val_loss: 1.2098 - val_acc: 0.6705\n",
      "Epoch 55/200\n",
      "5s - loss: 1.2103 - acc: 0.6690 - val_loss: 1.2100 - val_acc: 0.6713\n",
      "Epoch 56/200\n",
      "5s - loss: 1.2089 - acc: 0.6686 - val_loss: 1.2054 - val_acc: 0.6723\n",
      "Epoch 57/200\n",
      "5s - loss: 1.2063 - acc: 0.6698 - val_loss: 1.2055 - val_acc: 0.6707\n",
      "Epoch 58/200\n",
      "5s - loss: 1.2029 - acc: 0.6707 - val_loss: 1.1999 - val_acc: 0.6683\n",
      "Epoch 59/200\n",
      "5s - loss: 1.2001 - acc: 0.6715 - val_loss: 1.1962 - val_acc: 0.6712\n",
      "Epoch 60/200\n",
      "5s - loss: 1.1968 - acc: 0.6725 - val_loss: 1.1939 - val_acc: 0.6736\n",
      "Epoch 61/200\n",
      "5s - loss: 1.1956 - acc: 0.6720 - val_loss: 1.1945 - val_acc: 0.6706\n",
      "Epoch 62/200\n",
      "5s - loss: 1.1936 - acc: 0.6725 - val_loss: 1.1896 - val_acc: 0.6713\n",
      "Epoch 63/200\n",
      "5s - loss: 1.1911 - acc: 0.6726 - val_loss: 1.1892 - val_acc: 0.6772\n",
      "Epoch 64/200\n",
      "5s - loss: 1.1876 - acc: 0.6737 - val_loss: 1.1907 - val_acc: 0.6742\n",
      "Epoch 65/200\n",
      "5s - loss: 1.1853 - acc: 0.6742 - val_loss: 1.1837 - val_acc: 0.6713\n",
      "Epoch 66/200\n",
      "5s - loss: 1.1837 - acc: 0.6738 - val_loss: 1.1795 - val_acc: 0.6722\n",
      "Epoch 67/200\n",
      "5s - loss: 1.1817 - acc: 0.6742 - val_loss: 1.1803 - val_acc: 0.6751\n",
      "Epoch 68/200\n",
      "5s - loss: 1.1793 - acc: 0.6745 - val_loss: 1.1880 - val_acc: 0.6710\n",
      "Epoch 69/200\n",
      "5s - loss: 1.1771 - acc: 0.6753 - val_loss: 1.1749 - val_acc: 0.6751\n",
      "Epoch 70/200\n",
      "5s - loss: 1.1753 - acc: 0.6759 - val_loss: 1.1742 - val_acc: 0.6758\n",
      "Epoch 71/200\n",
      "5s - loss: 1.1723 - acc: 0.6763 - val_loss: 1.1707 - val_acc: 0.6738\n",
      "Epoch 72/200\n",
      "5s - loss: 1.1714 - acc: 0.6767 - val_loss: 1.1699 - val_acc: 0.6799\n",
      "Epoch 73/200\n",
      "5s - loss: 1.1688 - acc: 0.6769 - val_loss: 1.1659 - val_acc: 0.6771\n",
      "Epoch 74/200\n",
      "5s - loss: 1.1660 - acc: 0.6778 - val_loss: 1.1675 - val_acc: 0.6775\n",
      "Epoch 75/200\n",
      "5s - loss: 1.1657 - acc: 0.6778 - val_loss: 1.1646 - val_acc: 0.6809\n",
      "Epoch 76/200\n",
      "5s - loss: 1.1631 - acc: 0.6783 - val_loss: 1.1623 - val_acc: 0.6820\n",
      "Epoch 77/200\n",
      "5s - loss: 1.1608 - acc: 0.6794 - val_loss: 1.1606 - val_acc: 0.6789\n",
      "Epoch 78/200\n",
      "5s - loss: 1.1601 - acc: 0.6791 - val_loss: 1.1565 - val_acc: 0.6824\n",
      "Epoch 79/200\n",
      "5s - loss: 1.1579 - acc: 0.6799 - val_loss: 1.1720 - val_acc: 0.6787\n",
      "Epoch 80/200\n",
      "5s - loss: 1.1586 - acc: 0.6796 - val_loss: 1.1560 - val_acc: 0.6832\n",
      "Epoch 81/200\n",
      "5s - loss: 1.1542 - acc: 0.6809 - val_loss: 1.1518 - val_acc: 0.6831\n",
      "Epoch 82/200\n",
      "5s - loss: 1.1532 - acc: 0.6810 - val_loss: 1.1533 - val_acc: 0.6830\n",
      "Epoch 83/200\n",
      "5s - loss: 1.1529 - acc: 0.6811 - val_loss: 1.1522 - val_acc: 0.6805\n",
      "Epoch 84/200\n",
      "5s - loss: 1.1493 - acc: 0.6823 - val_loss: 1.1469 - val_acc: 0.6821\n",
      "Epoch 85/200\n",
      "5s - loss: 1.1483 - acc: 0.6825 - val_loss: 1.1531 - val_acc: 0.6841\n",
      "Epoch 86/200\n",
      "5s - loss: 1.1472 - acc: 0.6831 - val_loss: 1.1451 - val_acc: 0.6827\n",
      "Epoch 87/200\n",
      "5s - loss: 1.1462 - acc: 0.6835 - val_loss: 1.1435 - val_acc: 0.6831\n",
      "Epoch 88/200\n",
      "5s - loss: 1.1444 - acc: 0.6836 - val_loss: 1.1430 - val_acc: 0.6859\n",
      "Epoch 89/200\n",
      "5s - loss: 1.1427 - acc: 0.6838 - val_loss: 1.1468 - val_acc: 0.6756\n",
      "Epoch 90/200\n",
      "5s - loss: 1.1420 - acc: 0.6839 - val_loss: 1.1384 - val_acc: 0.6873\n",
      "Epoch 91/200\n",
      "5s - loss: 1.1415 - acc: 0.6841 - val_loss: 1.1367 - val_acc: 0.6864\n",
      "Epoch 92/200\n",
      "5s - loss: 1.1389 - acc: 0.6851 - val_loss: 1.1366 - val_acc: 0.6842\n",
      "Epoch 93/200\n",
      "5s - loss: 1.1365 - acc: 0.6854 - val_loss: 1.1390 - val_acc: 0.6865\n",
      "Epoch 94/200\n",
      "5s - loss: 1.1367 - acc: 0.6854 - val_loss: 1.1351 - val_acc: 0.6852\n",
      "Epoch 95/200\n",
      "5s - loss: 1.1354 - acc: 0.6857 - val_loss: 1.1367 - val_acc: 0.6858\n",
      "Epoch 96/200\n",
      "5s - loss: 1.1353 - acc: 0.6854 - val_loss: 1.1330 - val_acc: 0.6839\n",
      "Epoch 97/200\n",
      "5s - loss: 1.1316 - acc: 0.6868 - val_loss: 1.1310 - val_acc: 0.6885\n",
      "Epoch 98/200\n",
      "5s - loss: 1.1302 - acc: 0.6868 - val_loss: 1.1279 - val_acc: 0.6893\n",
      "Epoch 99/200\n",
      "5s - loss: 1.1315 - acc: 0.6867 - val_loss: 1.1285 - val_acc: 0.6872\n",
      "Epoch 100/200\n",
      "5s - loss: 1.1289 - acc: 0.6869 - val_loss: 1.1258 - val_acc: 0.6892\n",
      "Epoch 101/200\n",
      "5s - loss: 1.1272 - acc: 0.6877 - val_loss: 1.1271 - val_acc: 0.6877\n",
      "Epoch 102/200\n",
      "5s - loss: 1.1272 - acc: 0.6875 - val_loss: 1.1240 - val_acc: 0.6866\n",
      "Epoch 103/200\n",
      "5s - loss: 1.1264 - acc: 0.6879 - val_loss: 1.1238 - val_acc: 0.6900\n",
      "Epoch 104/200\n",
      "5s - loss: 1.1233 - acc: 0.6885 - val_loss: 1.1241 - val_acc: 0.6897\n",
      "Epoch 105/200\n",
      "5s - loss: 1.1221 - acc: 0.6890 - val_loss: 1.1200 - val_acc: 0.6904\n",
      "Epoch 106/200\n",
      "5s - loss: 1.1207 - acc: 0.6893 - val_loss: 1.1188 - val_acc: 0.6916\n",
      "Epoch 107/200\n",
      "5s - loss: 1.1200 - acc: 0.6891 - val_loss: 1.1177 - val_acc: 0.6904\n",
      "Epoch 108/200\n",
      "5s - loss: 1.1201 - acc: 0.6896 - val_loss: 1.1186 - val_acc: 0.6894\n",
      "Epoch 109/200\n",
      "5s - loss: 1.1181 - acc: 0.6899 - val_loss: 1.1189 - val_acc: 0.6901\n",
      "Epoch 110/200\n",
      "5s - loss: 1.1164 - acc: 0.6901 - val_loss: 1.1208 - val_acc: 0.6884\n",
      "Epoch 111/200\n",
      "5s - loss: 1.1156 - acc: 0.6904 - val_loss: 1.1132 - val_acc: 0.6903\n",
      "Epoch 112/200\n",
      "5s - loss: 1.1152 - acc: 0.6903 - val_loss: 1.1136 - val_acc: 0.6913\n",
      "Epoch 113/200\n",
      "5s - loss: 1.1147 - acc: 0.6902 - val_loss: 1.1134 - val_acc: 0.6910\n",
      "Epoch 114/200\n",
      "5s - loss: 1.1147 - acc: 0.6907 - val_loss: 1.1175 - val_acc: 0.6894\n",
      "Epoch 115/200\n",
      "5s - loss: 1.1125 - acc: 0.6910 - val_loss: 1.1106 - val_acc: 0.6913\n",
      "Epoch 116/200\n",
      "5s - loss: 1.1110 - acc: 0.6916 - val_loss: 1.1116 - val_acc: 0.6926\n",
      "Epoch 117/200\n",
      "5s - loss: 1.1107 - acc: 0.6915 - val_loss: 1.1072 - val_acc: 0.6921\n",
      "Epoch 118/200\n",
      "5s - loss: 1.1098 - acc: 0.6915 - val_loss: 1.1086 - val_acc: 0.6929\n",
      "Epoch 119/200\n",
      "5s - loss: 1.1075 - acc: 0.6922 - val_loss: 1.1115 - val_acc: 0.6925\n",
      "Epoch 120/200\n",
      "5s - loss: 1.1072 - acc: 0.6921 - val_loss: 1.1078 - val_acc: 0.6915\n",
      "Epoch 121/200\n",
      "5s - loss: 1.1099 - acc: 0.6913 - val_loss: 1.1299 - val_acc: 0.6873\n",
      "Epoch 122/200\n",
      "5s - loss: 1.1090 - acc: 0.6916 - val_loss: 1.1036 - val_acc: 0.6938\n",
      "Epoch 123/200\n",
      "5s - loss: 1.1046 - acc: 0.6929 - val_loss: 1.1050 - val_acc: 0.6940\n",
      "Epoch 124/200\n",
      "5s - loss: 1.1028 - acc: 0.6936 - val_loss: 1.1023 - val_acc: 0.6944\n",
      "Epoch 125/200\n",
      "5s - loss: 1.1030 - acc: 0.6932 - val_loss: 1.1004 - val_acc: 0.6950\n",
      "Epoch 126/200\n",
      "5s - loss: 1.1019 - acc: 0.6939 - val_loss: 1.0996 - val_acc: 0.6946\n",
      "Epoch 127/200\n",
      "5s - loss: 1.1014 - acc: 0.6941 - val_loss: 1.0987 - val_acc: 0.6966\n",
      "Epoch 128/200\n",
      "5s - loss: 1.0998 - acc: 0.6944 - val_loss: 1.1042 - val_acc: 0.6940\n",
      "Epoch 129/200\n",
      "5s - loss: 1.0997 - acc: 0.6951 - val_loss: 1.0974 - val_acc: 0.6965\n",
      "Epoch 130/200\n",
      "5s - loss: 1.0983 - acc: 0.6956 - val_loss: 1.0951 - val_acc: 0.6961\n",
      "Epoch 131/200\n",
      "5s - loss: 1.0969 - acc: 0.6962 - val_loss: 1.0977 - val_acc: 0.6961\n",
      "Epoch 132/200\n",
      "5s - loss: 1.0966 - acc: 0.6960 - val_loss: 1.1046 - val_acc: 0.6957\n",
      "Epoch 133/200\n",
      "5s - loss: 1.0967 - acc: 0.6964 - val_loss: 1.1073 - val_acc: 0.6952\n",
      "Epoch 134/200\n",
      "5s - loss: 1.0949 - acc: 0.6969 - val_loss: 1.0929 - val_acc: 0.6975\n",
      "Epoch 135/200\n",
      "5s - loss: 1.0933 - acc: 0.6974 - val_loss: 1.0951 - val_acc: 0.6970\n",
      "Epoch 136/200\n",
      "5s - loss: 1.0948 - acc: 0.6966 - val_loss: 1.0898 - val_acc: 0.6997\n",
      "Epoch 137/200\n",
      "5s - loss: 1.0938 - acc: 0.6969 - val_loss: 1.0893 - val_acc: 0.6974\n",
      "Epoch 138/200\n",
      "5s - loss: 1.0921 - acc: 0.6974 - val_loss: 1.0923 - val_acc: 0.6985\n",
      "Epoch 139/200\n",
      "5s - loss: 1.0915 - acc: 0.6977 - val_loss: 1.0887 - val_acc: 0.6980\n",
      "Epoch 140/200\n",
      "5s - loss: 1.0898 - acc: 0.6982 - val_loss: 1.0936 - val_acc: 0.6957\n",
      "Epoch 141/200\n",
      "5s - loss: 1.0911 - acc: 0.6977 - val_loss: 1.0891 - val_acc: 0.6982\n",
      "Epoch 142/200\n",
      "5s - loss: 1.0881 - acc: 0.6985 - val_loss: 1.0862 - val_acc: 0.6994\n",
      "Epoch 143/200\n",
      "5s - loss: 1.0863 - acc: 0.6990 - val_loss: 1.0873 - val_acc: 0.6978\n",
      "Epoch 144/200\n",
      "5s - loss: 1.0880 - acc: 0.6984 - val_loss: 1.0853 - val_acc: 0.6994\n",
      "Epoch 145/200\n",
      "5s - loss: 1.0873 - acc: 0.6983 - val_loss: 1.0854 - val_acc: 0.6984\n",
      "Epoch 146/200\n",
      "5s - loss: 1.0866 - acc: 0.6987 - val_loss: 1.0886 - val_acc: 0.6976\n",
      "Epoch 147/200\n",
      "5s - loss: 1.0838 - acc: 0.6994 - val_loss: 1.0827 - val_acc: 0.6990\n",
      "Epoch 148/200\n",
      "5s - loss: 1.0848 - acc: 0.6991 - val_loss: 1.0828 - val_acc: 0.7002\n",
      "Epoch 149/200\n",
      "5s - loss: 1.0844 - acc: 0.6990 - val_loss: 1.0832 - val_acc: 0.7007\n",
      "Epoch 150/200\n",
      "5s - loss: 1.0830 - acc: 0.6994 - val_loss: 1.0825 - val_acc: 0.6996\n",
      "Epoch 151/200\n",
      "5s - loss: 1.0847 - acc: 0.6988 - val_loss: 1.0831 - val_acc: 0.6999\n",
      "Epoch 152/200\n",
      "5s - loss: 1.0811 - acc: 0.6997 - val_loss: 1.0811 - val_acc: 0.7000\n",
      "Epoch 153/200\n",
      "5s - loss: 1.0826 - acc: 0.6995 - val_loss: 1.0838 - val_acc: 0.6995\n",
      "Epoch 154/200\n",
      "5s - loss: 1.0802 - acc: 0.6998 - val_loss: 1.0803 - val_acc: 0.6997\n",
      "Epoch 155/200\n",
      "5s - loss: 1.0799 - acc: 0.6999 - val_loss: 1.0799 - val_acc: 0.7009\n",
      "Epoch 156/200\n",
      "5s - loss: 1.0795 - acc: 0.7000 - val_loss: 1.0857 - val_acc: 0.6988\n",
      "Epoch 157/200\n",
      "5s - loss: 1.0786 - acc: 0.7001 - val_loss: 1.0777 - val_acc: 0.7015\n",
      "Epoch 158/200\n",
      "5s - loss: 1.0773 - acc: 0.7007 - val_loss: 1.0823 - val_acc: 0.6995\n",
      "Epoch 159/200\n",
      "5s - loss: 1.0773 - acc: 0.7005 - val_loss: 1.0772 - val_acc: 0.6997\n",
      "Epoch 160/200\n",
      "5s - loss: 1.0763 - acc: 0.7005 - val_loss: 1.0758 - val_acc: 0.7009\n",
      "Epoch 161/200\n",
      "5s - loss: 1.0765 - acc: 0.7008 - val_loss: 1.0767 - val_acc: 0.7008\n",
      "Epoch 162/200\n",
      "5s - loss: 1.0745 - acc: 0.7012 - val_loss: 1.0738 - val_acc: 0.7015\n",
      "Epoch 163/200\n",
      "5s - loss: 1.0755 - acc: 0.7009 - val_loss: 1.0757 - val_acc: 0.7003\n",
      "Epoch 164/200\n",
      "5s - loss: 1.0739 - acc: 0.7013 - val_loss: 1.0730 - val_acc: 0.7014\n",
      "Epoch 165/200\n",
      "5s - loss: 1.0742 - acc: 0.7012 - val_loss: 1.0722 - val_acc: 0.7015\n",
      "Epoch 166/200\n",
      "5s - loss: 1.0753 - acc: 0.7010 - val_loss: 1.0711 - val_acc: 0.7015\n",
      "Epoch 167/200\n",
      "5s - loss: 1.0740 - acc: 0.7009 - val_loss: 1.0736 - val_acc: 0.7009\n",
      "Epoch 168/200\n",
      "5s - loss: 1.0705 - acc: 0.7019 - val_loss: 1.0715 - val_acc: 0.7034\n",
      "Epoch 169/200\n",
      "5s - loss: 1.0711 - acc: 0.7017 - val_loss: 1.0750 - val_acc: 0.7011\n",
      "Epoch 170/200\n",
      "5s - loss: 1.0719 - acc: 0.7013 - val_loss: 1.0683 - val_acc: 0.7021\n",
      "Epoch 171/200\n",
      "5s - loss: 1.0701 - acc: 0.7020 - val_loss: 1.0710 - val_acc: 0.7026\n",
      "Epoch 172/200\n",
      "5s - loss: 1.0694 - acc: 0.7023 - val_loss: 1.0679 - val_acc: 0.7025\n",
      "Epoch 173/200\n",
      "5s - loss: 1.0697 - acc: 0.7017 - val_loss: 1.0685 - val_acc: 0.7013\n",
      "Epoch 174/200\n",
      "5s - loss: 1.0684 - acc: 0.7025 - val_loss: 1.0684 - val_acc: 0.7024\n",
      "Epoch 175/200\n",
      "5s - loss: 1.0677 - acc: 0.7026 - val_loss: 1.0697 - val_acc: 0.7014\n",
      "Epoch 176/200\n",
      "5s - loss: 1.0678 - acc: 0.7024 - val_loss: 1.0677 - val_acc: 0.7022\n",
      "Epoch 177/200\n",
      "5s - loss: 1.0666 - acc: 0.7030 - val_loss: 1.0690 - val_acc: 0.7012\n",
      "Epoch 178/200\n",
      "5s - loss: 1.0642 - acc: 0.7033 - val_loss: 1.0679 - val_acc: 0.7035\n",
      "Epoch 179/200\n",
      "5s - loss: 1.0653 - acc: 0.7032 - val_loss: 1.0649 - val_acc: 0.7031\n",
      "Epoch 180/200\n",
      "5s - loss: 1.0655 - acc: 0.7028 - val_loss: 1.0616 - val_acc: 0.7029\n",
      "Epoch 181/200\n",
      "5s - loss: 1.0641 - acc: 0.7034 - val_loss: 1.0662 - val_acc: 0.7010\n",
      "Epoch 182/200\n",
      "5s - loss: 1.0644 - acc: 0.7032 - val_loss: 1.0643 - val_acc: 0.7021\n",
      "Epoch 183/200\n",
      "5s - loss: 1.0629 - acc: 0.7038 - val_loss: 1.0620 - val_acc: 0.7040\n",
      "Epoch 184/200\n",
      "5s - loss: 1.0636 - acc: 0.7033 - val_loss: 1.0642 - val_acc: 0.7026\n",
      "Epoch 185/200\n",
      "5s - loss: 1.0618 - acc: 0.7039 - val_loss: 1.0626 - val_acc: 0.7041\n",
      "Epoch 186/200\n",
      "5s - loss: 1.0611 - acc: 0.7038 - val_loss: 1.0590 - val_acc: 0.7049\n",
      "Epoch 187/200\n",
      "5s - loss: 1.0597 - acc: 0.7043 - val_loss: 1.0593 - val_acc: 0.7046\n",
      "Epoch 188/200\n",
      "5s - loss: 1.0599 - acc: 0.7043 - val_loss: 1.0603 - val_acc: 0.7036\n",
      "Epoch 189/200\n",
      "5s - loss: 1.0599 - acc: 0.7043 - val_loss: 1.0576 - val_acc: 0.7061\n",
      "Epoch 190/200\n",
      "5s - loss: 1.0594 - acc: 0.7041 - val_loss: 1.0669 - val_acc: 0.6991\n",
      "Epoch 191/200\n",
      "5s - loss: 1.0579 - acc: 0.7046 - val_loss: 1.0586 - val_acc: 0.7052\n",
      "Epoch 192/200\n",
      "5s - loss: 1.0569 - acc: 0.7045 - val_loss: 1.0579 - val_acc: 0.7054\n",
      "Epoch 193/200\n",
      "5s - loss: 1.0577 - acc: 0.7045 - val_loss: 1.0625 - val_acc: 0.7014\n",
      "Epoch 194/200\n",
      "5s - loss: 1.0569 - acc: 0.7048 - val_loss: 1.0530 - val_acc: 0.7067\n",
      "Epoch 195/200\n",
      "5s - loss: 1.0553 - acc: 0.7049 - val_loss: 1.0573 - val_acc: 0.7055\n",
      "Epoch 196/200\n",
      "5s - loss: 1.0546 - acc: 0.7052 - val_loss: 1.0532 - val_acc: 0.7055\n",
      "Epoch 197/200\n",
      "5s - loss: 1.0541 - acc: 0.7049 - val_loss: 1.0532 - val_acc: 0.7062\n",
      "Epoch 198/200\n",
      "5s - loss: 1.0529 - acc: 0.7053 - val_loss: 1.0541 - val_acc: 0.7062\n",
      "Epoch 199/200\n",
      "5s - loss: 1.0533 - acc: 0.7054 - val_loss: 1.0611 - val_acc: 0.7050\n",
      "Epoch 200/200\n",
      "5s - loss: 1.0513 - acc: 0.7059 - val_loss: 1.0492 - val_acc: 0.7057\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noSAiIj0UCiIi0uP/Ay4huHux4vpVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2510ca87780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = History()                 #need this in order to do my vsualizations\n",
    "\n",
    "\n",
    "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a basic RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Build the layers -- Done!\n",
    "    \"\"\"\n",
    "    Keras components/tools ...\n",
    "    Sequential       -- Linear model that we want to add RNN layers to.\n",
    "    GRU              -- Gated recurrent unit/layer (used to improve RNN models)\n",
    "    TimeDistributed  -- Wrapper for applying timesteps independently to dense layers\n",
    "    Dense            -- Basic densely-connected NN layer\n",
    "    Activation       -- Applies an activation function to an output (e.g. softmax, relu, sigmoid, tanh and linear)\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(GRU(output_sequence_length, input_shape=input_shape[1:], return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size)))\n",
    "    model.add(Activation('softmax'))\n",
    " \n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(lr=0.005), \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "tests.test_simple_model(simple_model)\n",
    "\n",
    "\n",
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "simple_rnn_model = simple_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1) #hmm...Was getting \"val_loss: nan\" without the \"+1\"\n",
    "simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=200, validation_split=0.2, verbose=2, callbacks=[history])\n",
    "#set verbose=2 because the progress bar was causing the GPU to croak\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('simple model loss - 5 epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train loss', 'validation loss'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#batch = 64   => 45s/batch Epoch 5/5 -45s - loss: 1.2744 - acc: 0.6514 - val_loss: 1.2601 - val_acc: 0.6528\n",
    "#batch = 256  => 12s/batch Epoch 5/5 -12s - loss: 1.4551 - acc: 0.6063 - val_loss: 1.4315 - val_acc: 0.6091\n",
    "#batch = 512  => 8s/batch  Epoch 5/5  -7s - loss: 1.5899 - acc: 0.5917 - val_loss: 1.5655 - val_acc: 0.5979\n",
    "#batch = 1024 => 6s/batch  Epoch 5/5  -5s - loss: 1.7518 - acc: 0.5727 - val_loss: 1.7177 - val_acc: 0.5734\n",
    "#seems the higher the number of batches the quicker the processing and the better the accuracy, \n",
    "#so for now we'll go with batch = 1024 unless we observe overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![RNN](simple_model_5_epochs.png)![RNN](simple_model_200_epochs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "Prediction:\n",
      "new jersey est est jamais en en mais il est en en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "# Print prediction(s)\n",
    "print(english_sentences[0])\n",
    "print(french_sentences[0])\n",
    "print(\"Prediction:\")\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model 2: Embedding (IMPLEMENTATION)\n",
    "![RNN](images/embedding.png)\n",
    "You've turned the words into ids, but there's a better representation of a word.  This is called word embeddings.  An embedding is a vector representation of the word that is close to similar words in n-dimensional space, where the n represents the size of the embedding vectors.\n",
    "\n",
    "In this model, you'll create a RNN model using embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 21, 346)           69200     \n",
      "_________________________________________________________________\n",
      "gru_30 (GRU)                 (None, 21, 21)            23184     \n",
      "_________________________________________________________________\n",
      "time_distributed_30 (TimeDis (None, 21, 346)           7612      \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 21, 346)           0         \n",
      "=================================================================\n",
      "Total params: 99,996\n",
      "Trainable params: 99,996\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/200\n",
      "12s - loss: 4.8242 - acc: 0.1523 - val_loss: 4.0385 - val_acc: 0.1670\n",
      "Epoch 2/200\n",
      "10s - loss: 3.6006 - acc: 0.2297 - val_loss: 3.1452 - val_acc: 0.3006\n",
      "Epoch 3/200\n",
      "10s - loss: 2.7925 - acc: 0.3873 - val_loss: 2.4877 - val_acc: 0.4478\n",
      "Epoch 4/200\n",
      "10s - loss: 2.2855 - acc: 0.4672 - val_loss: 2.1089 - val_acc: 0.4778\n",
      "Epoch 5/200\n",
      "10s - loss: 1.9858 - acc: 0.4873 - val_loss: 1.8695 - val_acc: 0.4972\n",
      "Epoch 6/200\n",
      "10s - loss: 1.7808 - acc: 0.5102 - val_loss: 1.6963 - val_acc: 0.5296\n",
      "Epoch 7/200\n",
      "10s - loss: 1.6308 - acc: 0.5500 - val_loss: 1.5670 - val_acc: 0.5641\n",
      "Epoch 8/200\n",
      "10s - loss: 1.5182 - acc: 0.5733 - val_loss: 1.4666 - val_acc: 0.5801\n",
      "Epoch 9/200\n",
      "10s - loss: 1.4248 - acc: 0.5887 - val_loss: 1.3783 - val_acc: 0.5999\n",
      "Epoch 10/200\n",
      "10s - loss: 1.3412 - acc: 0.6076 - val_loss: 1.3031 - val_acc: 0.6176\n",
      "Epoch 11/200\n",
      "10s - loss: 1.2749 - acc: 0.6234 - val_loss: 1.2431 - val_acc: 0.6311\n",
      "Epoch 12/200\n",
      "10s - loss: 1.2161 - acc: 0.6413 - val_loss: 1.1866 - val_acc: 0.6492\n",
      "Epoch 13/200\n",
      "10s - loss: 1.1582 - acc: 0.6626 - val_loss: 1.1263 - val_acc: 0.6726\n",
      "Epoch 14/200\n",
      "10s - loss: 1.1036 - acc: 0.6831 - val_loss: 1.0780 - val_acc: 0.6907\n",
      "Epoch 15/200\n",
      "10s - loss: 1.0590 - acc: 0.6979 - val_loss: 1.0368 - val_acc: 0.7050\n",
      "Epoch 16/200\n",
      "10s - loss: 1.0224 - acc: 0.7107 - val_loss: 1.0026 - val_acc: 0.7170\n",
      "Epoch 17/200\n",
      "10s - loss: 0.9903 - acc: 0.7206 - val_loss: 0.9739 - val_acc: 0.7262\n",
      "Epoch 18/200\n",
      "10s - loss: 0.9616 - acc: 0.7279 - val_loss: 0.9474 - val_acc: 0.7310\n",
      "Epoch 19/200\n",
      "10s - loss: 0.9355 - acc: 0.7349 - val_loss: 0.9234 - val_acc: 0.7377\n",
      "Epoch 20/200\n",
      "10s - loss: 0.9132 - acc: 0.7407 - val_loss: 0.9015 - val_acc: 0.7438\n",
      "Epoch 21/200\n",
      "10s - loss: 0.8947 - acc: 0.7455 - val_loss: 0.8825 - val_acc: 0.7493\n",
      "Epoch 22/200\n",
      "10s - loss: 0.8764 - acc: 0.7503 - val_loss: 0.8652 - val_acc: 0.7532\n",
      "Epoch 23/200\n",
      "10s - loss: 0.8596 - acc: 0.7545 - val_loss: 0.8530 - val_acc: 0.7570\n",
      "Epoch 24/200\n",
      "10s - loss: 0.8438 - acc: 0.7586 - val_loss: 0.8366 - val_acc: 0.7602\n",
      "Epoch 25/200\n",
      "10s - loss: 0.8298 - acc: 0.7621 - val_loss: 0.8236 - val_acc: 0.7618\n",
      "Epoch 26/200\n",
      "10s - loss: 0.8162 - acc: 0.7654 - val_loss: 0.8108 - val_acc: 0.7649\n",
      "Epoch 27/200\n",
      "10s - loss: 0.8064 - acc: 0.7674 - val_loss: 0.8023 - val_acc: 0.7673\n",
      "Epoch 28/200\n",
      "10s - loss: 0.7933 - acc: 0.7709 - val_loss: 0.7884 - val_acc: 0.7717\n",
      "Epoch 29/200\n",
      "10s - loss: 0.7838 - acc: 0.7737 - val_loss: 0.7811 - val_acc: 0.7744\n",
      "Epoch 30/200\n",
      "10s - loss: 0.7744 - acc: 0.7765 - val_loss: 0.7728 - val_acc: 0.7778\n",
      "Epoch 31/200\n",
      "10s - loss: 0.7660 - acc: 0.7792 - val_loss: 0.7650 - val_acc: 0.7797\n",
      "Epoch 32/200\n",
      "10s - loss: 0.7559 - acc: 0.7822 - val_loss: 0.7534 - val_acc: 0.7843\n",
      "Epoch 33/200\n",
      "10s - loss: 0.7476 - acc: 0.7848 - val_loss: 0.7482 - val_acc: 0.7837\n",
      "Epoch 34/200\n",
      "10s - loss: 0.7410 - acc: 0.7862 - val_loss: 0.7394 - val_acc: 0.7872\n",
      "Epoch 35/200\n",
      "10s - loss: 0.7326 - acc: 0.7883 - val_loss: 0.7325 - val_acc: 0.7881\n",
      "Epoch 36/200\n",
      "10s - loss: 0.7256 - acc: 0.7907 - val_loss: 0.7271 - val_acc: 0.7900\n",
      "Epoch 37/200\n",
      "10s - loss: 0.7190 - acc: 0.7921 - val_loss: 0.7218 - val_acc: 0.7915\n",
      "Epoch 38/200\n",
      "10s - loss: 0.7129 - acc: 0.7935 - val_loss: 0.7143 - val_acc: 0.7945\n",
      "Epoch 39/200\n",
      "10s - loss: 0.7070 - acc: 0.7953 - val_loss: 0.7072 - val_acc: 0.7963\n",
      "Epoch 40/200\n",
      "10s - loss: 0.7004 - acc: 0.7968 - val_loss: 0.7044 - val_acc: 0.7971\n",
      "Epoch 41/200\n",
      "10s - loss: 0.6960 - acc: 0.7976 - val_loss: 0.6994 - val_acc: 0.7971\n",
      "Epoch 42/200\n",
      "10s - loss: 0.6911 - acc: 0.7986 - val_loss: 0.6947 - val_acc: 0.7983\n",
      "Epoch 43/200\n",
      "10s - loss: 0.6861 - acc: 0.7998 - val_loss: 0.6889 - val_acc: 0.7987\n",
      "Epoch 44/200\n",
      "10s - loss: 0.6809 - acc: 0.8008 - val_loss: 0.6837 - val_acc: 0.8011\n",
      "Epoch 45/200\n",
      "10s - loss: 0.6769 - acc: 0.8019 - val_loss: 0.6788 - val_acc: 0.8024\n",
      "Epoch 46/200\n",
      "10s - loss: 0.6713 - acc: 0.8031 - val_loss: 0.6766 - val_acc: 0.8030\n",
      "Epoch 47/200\n",
      "10s - loss: 0.6666 - acc: 0.8043 - val_loss: 0.6752 - val_acc: 0.8023\n",
      "Epoch 48/200\n",
      "10s - loss: 0.6637 - acc: 0.8049 - val_loss: 0.6673 - val_acc: 0.8052\n",
      "Epoch 49/200\n",
      "10s - loss: 0.6590 - acc: 0.8061 - val_loss: 0.6635 - val_acc: 0.8062\n",
      "Epoch 50/200\n",
      "10s - loss: 0.6551 - acc: 0.8074 - val_loss: 0.6601 - val_acc: 0.8065\n",
      "Epoch 51/200\n",
      "10s - loss: 0.6522 - acc: 0.8077 - val_loss: 0.6566 - val_acc: 0.8077\n",
      "Epoch 52/200\n",
      "10s - loss: 0.6479 - acc: 0.8089 - val_loss: 0.6555 - val_acc: 0.8068\n",
      "Epoch 53/200\n",
      "10s - loss: 0.6469 - acc: 0.8089 - val_loss: 0.6502 - val_acc: 0.8082\n",
      "Epoch 54/200\n",
      "10s - loss: 0.6405 - acc: 0.8107 - val_loss: 0.6442 - val_acc: 0.8100\n",
      "Epoch 55/200\n",
      "10s - loss: 0.6390 - acc: 0.8110 - val_loss: 0.6437 - val_acc: 0.8101\n",
      "Epoch 56/200\n",
      "10s - loss: 0.6361 - acc: 0.8117 - val_loss: 0.6393 - val_acc: 0.8114\n",
      "Epoch 57/200\n",
      "10s - loss: 0.6332 - acc: 0.8121 - val_loss: 0.6404 - val_acc: 0.8113\n",
      "Epoch 58/200\n",
      "10s - loss: 0.6301 - acc: 0.8132 - val_loss: 0.6353 - val_acc: 0.8126\n",
      "Epoch 59/200\n",
      "10s - loss: 0.6268 - acc: 0.8139 - val_loss: 0.6330 - val_acc: 0.8117\n",
      "Epoch 60/200\n",
      "10s - loss: 0.6254 - acc: 0.8140 - val_loss: 0.6317 - val_acc: 0.8119\n",
      "Epoch 61/200\n",
      "10s - loss: 0.6232 - acc: 0.8145 - val_loss: 0.6287 - val_acc: 0.8142\n",
      "Epoch 62/200\n",
      "10s - loss: 0.6196 - acc: 0.8159 - val_loss: 0.6270 - val_acc: 0.8150\n",
      "Epoch 63/200\n",
      "10s - loss: 0.6168 - acc: 0.8161 - val_loss: 0.6211 - val_acc: 0.8163\n",
      "Epoch 64/200\n",
      "10s - loss: 0.6143 - acc: 0.8170 - val_loss: 0.6213 - val_acc: 0.8154\n",
      "Epoch 65/200\n",
      "10s - loss: 0.6140 - acc: 0.8169 - val_loss: 0.6200 - val_acc: 0.8156\n",
      "Epoch 66/200\n",
      "10s - loss: 0.6136 - acc: 0.8168 - val_loss: 0.6161 - val_acc: 0.8175\n",
      "Epoch 67/200\n",
      "10s - loss: 0.6092 - acc: 0.8182 - val_loss: 0.6153 - val_acc: 0.8175\n",
      "Epoch 68/200\n",
      "10s - loss: 0.6056 - acc: 0.8192 - val_loss: 0.6132 - val_acc: 0.8175\n",
      "Epoch 69/200\n",
      "10s - loss: 0.6045 - acc: 0.8195 - val_loss: 0.6082 - val_acc: 0.8196\n",
      "Epoch 70/200\n",
      "10s - loss: 0.6016 - acc: 0.8202 - val_loss: 0.6061 - val_acc: 0.8210\n",
      "Epoch 71/200\n",
      "10s - loss: 0.6003 - acc: 0.8205 - val_loss: 0.6071 - val_acc: 0.8189\n",
      "Epoch 72/200\n",
      "11s - loss: 0.5990 - acc: 0.8210 - val_loss: 0.6102 - val_acc: 0.8193\n",
      "Epoch 73/200\n",
      "10s - loss: 0.5978 - acc: 0.8211 - val_loss: 0.6031 - val_acc: 0.8207\n",
      "Epoch 74/200\n",
      "10s - loss: 0.5961 - acc: 0.8220 - val_loss: 0.6036 - val_acc: 0.8204\n",
      "Epoch 75/200\n",
      "10s - loss: 0.5933 - acc: 0.8225 - val_loss: 0.5982 - val_acc: 0.8215\n",
      "Epoch 76/200\n",
      "10s - loss: 0.5924 - acc: 0.8225 - val_loss: 0.5997 - val_acc: 0.8210\n",
      "Epoch 77/200\n",
      "10s - loss: 0.5915 - acc: 0.8227 - val_loss: 0.5963 - val_acc: 0.8227\n",
      "Epoch 78/200\n",
      "10s - loss: 0.5901 - acc: 0.8232 - val_loss: 0.5970 - val_acc: 0.8223\n",
      "Epoch 79/200\n",
      "10s - loss: 0.5874 - acc: 0.8240 - val_loss: 0.5936 - val_acc: 0.8232\n",
      "Epoch 80/200\n",
      "10s - loss: 0.5871 - acc: 0.8240 - val_loss: 0.5929 - val_acc: 0.8233\n",
      "Epoch 81/200\n",
      "10s - loss: 0.5853 - acc: 0.8243 - val_loss: 0.5928 - val_acc: 0.8236\n",
      "Epoch 82/200\n",
      "10s - loss: 0.5835 - acc: 0.8251 - val_loss: 0.5897 - val_acc: 0.8247\n",
      "Epoch 83/200\n",
      "10s - loss: 0.5822 - acc: 0.8258 - val_loss: 0.5892 - val_acc: 0.8251\n",
      "Epoch 84/200\n",
      "10s - loss: 0.5821 - acc: 0.8254 - val_loss: 0.5911 - val_acc: 0.8253\n",
      "Epoch 85/200\n",
      "10s - loss: 0.5812 - acc: 0.8258 - val_loss: 0.5884 - val_acc: 0.8245\n",
      "Epoch 86/200\n",
      "10s - loss: 0.5788 - acc: 0.8267 - val_loss: 0.5856 - val_acc: 0.8259\n",
      "Epoch 87/200\n",
      "10s - loss: 0.5787 - acc: 0.8269 - val_loss: 0.5848 - val_acc: 0.8259\n",
      "Epoch 88/200\n",
      "10s - loss: 0.5770 - acc: 0.8275 - val_loss: 0.5850 - val_acc: 0.8262\n",
      "Epoch 89/200\n",
      "10s - loss: 0.5757 - acc: 0.8278 - val_loss: 0.5832 - val_acc: 0.8268\n",
      "Epoch 90/200\n",
      "10s - loss: 0.5752 - acc: 0.8279 - val_loss: 0.5797 - val_acc: 0.8271\n",
      "Epoch 91/200\n",
      "10s - loss: 0.5740 - acc: 0.8281 - val_loss: 0.5818 - val_acc: 0.8273\n",
      "Epoch 92/200\n",
      "10s - loss: 0.5730 - acc: 0.8287 - val_loss: 0.5807 - val_acc: 0.8275\n",
      "Epoch 93/200\n",
      "10s - loss: 0.5716 - acc: 0.8288 - val_loss: 0.5788 - val_acc: 0.8283\n",
      "Epoch 94/200\n",
      "10s - loss: 0.5706 - acc: 0.8291 - val_loss: 0.5806 - val_acc: 0.8273\n",
      "Epoch 95/200\n",
      "10s - loss: 0.5695 - acc: 0.8295 - val_loss: 0.5771 - val_acc: 0.8287\n",
      "Epoch 96/200\n",
      "10s - loss: 0.5701 - acc: 0.8292 - val_loss: 0.5784 - val_acc: 0.8286\n",
      "Epoch 97/200\n",
      "10s - loss: 0.5679 - acc: 0.8299 - val_loss: 0.5777 - val_acc: 0.8282\n",
      "Epoch 98/200\n",
      "10s - loss: 0.5661 - acc: 0.8303 - val_loss: 0.5716 - val_acc: 0.8298\n",
      "Epoch 99/200\n",
      "10s - loss: 0.5652 - acc: 0.8304 - val_loss: 0.5745 - val_acc: 0.8286\n",
      "Epoch 100/200\n",
      "10s - loss: 0.5653 - acc: 0.8306 - val_loss: 0.5731 - val_acc: 0.8285\n",
      "Epoch 101/200\n",
      "10s - loss: 0.5642 - acc: 0.8309 - val_loss: 0.5717 - val_acc: 0.8295\n",
      "Epoch 102/200\n",
      "10s - loss: 0.5628 - acc: 0.8311 - val_loss: 0.5715 - val_acc: 0.8293\n",
      "Epoch 103/200\n",
      "10s - loss: 0.5617 - acc: 0.8314 - val_loss: 0.5686 - val_acc: 0.8306\n",
      "Epoch 104/200\n",
      "10s - loss: 0.5608 - acc: 0.8319 - val_loss: 0.5707 - val_acc: 0.8303\n",
      "Epoch 105/200\n",
      "10s - loss: 0.5599 - acc: 0.8316 - val_loss: 0.5705 - val_acc: 0.8300\n",
      "Epoch 106/200\n",
      "10s - loss: 0.5608 - acc: 0.8315 - val_loss: 0.5683 - val_acc: 0.8308\n",
      "Epoch 107/200\n",
      "10s - loss: 0.5589 - acc: 0.8323 - val_loss: 0.5700 - val_acc: 0.8300\n",
      "Epoch 108/200\n",
      "10s - loss: 0.5573 - acc: 0.8327 - val_loss: 0.5670 - val_acc: 0.8306\n",
      "Epoch 109/200\n",
      "10s - loss: 0.5570 - acc: 0.8326 - val_loss: 0.5654 - val_acc: 0.8318\n",
      "Epoch 110/200\n",
      "10s - loss: 0.5577 - acc: 0.8324 - val_loss: 0.5654 - val_acc: 0.8316\n",
      "Epoch 111/200\n",
      "10s - loss: 0.5558 - acc: 0.8328 - val_loss: 0.5638 - val_acc: 0.8321\n",
      "Epoch 112/200\n",
      "10s - loss: 0.5558 - acc: 0.8330 - val_loss: 0.5636 - val_acc: 0.8311\n",
      "Epoch 113/200\n",
      "10s - loss: 0.5543 - acc: 0.8333 - val_loss: 0.5654 - val_acc: 0.8306\n",
      "Epoch 114/200\n",
      "10s - loss: 0.5536 - acc: 0.8334 - val_loss: 0.5602 - val_acc: 0.8329\n",
      "Epoch 115/200\n",
      "10s - loss: 0.5523 - acc: 0.8339 - val_loss: 0.5612 - val_acc: 0.8311\n",
      "Epoch 116/200\n",
      "10s - loss: 0.5525 - acc: 0.8339 - val_loss: 0.5649 - val_acc: 0.8314\n",
      "Epoch 117/200\n",
      "10s - loss: 0.5514 - acc: 0.8340 - val_loss: 0.5603 - val_acc: 0.8329\n",
      "Epoch 118/200\n",
      "10s - loss: 0.5522 - acc: 0.8342 - val_loss: 0.5604 - val_acc: 0.8327\n",
      "Epoch 119/200\n",
      "10s - loss: 0.5523 - acc: 0.8340 - val_loss: 0.5607 - val_acc: 0.8324\n",
      "Epoch 120/200\n",
      "10s - loss: 0.5493 - acc: 0.8347 - val_loss: 0.5591 - val_acc: 0.8325\n",
      "Epoch 121/200\n",
      "10s - loss: 0.5494 - acc: 0.8348 - val_loss: 0.5584 - val_acc: 0.8334\n",
      "Epoch 122/200\n",
      "10s - loss: 0.5482 - acc: 0.8353 - val_loss: 0.5561 - val_acc: 0.8351\n",
      "Epoch 123/200\n",
      "10s - loss: 0.5477 - acc: 0.8354 - val_loss: 0.5560 - val_acc: 0.8345\n",
      "Epoch 124/200\n",
      "10s - loss: 0.5473 - acc: 0.8352 - val_loss: 0.5576 - val_acc: 0.8341\n",
      "Epoch 125/200\n",
      "10s - loss: 0.5464 - acc: 0.8357 - val_loss: 0.5567 - val_acc: 0.8344\n",
      "Epoch 126/200\n",
      "10s - loss: 0.5445 - acc: 0.8362 - val_loss: 0.5567 - val_acc: 0.8340\n",
      "Epoch 127/200\n",
      "10s - loss: 0.5449 - acc: 0.8362 - val_loss: 0.5556 - val_acc: 0.8346\n",
      "Epoch 128/200\n",
      "10s - loss: 0.5456 - acc: 0.8358 - val_loss: 0.5589 - val_acc: 0.8327\n",
      "Epoch 129/200\n",
      "10s - loss: 0.5452 - acc: 0.8361 - val_loss: 0.5554 - val_acc: 0.8342\n",
      "Epoch 130/200\n",
      "10s - loss: 0.5429 - acc: 0.8370 - val_loss: 0.5521 - val_acc: 0.8350\n",
      "Epoch 131/200\n",
      "10s - loss: 0.5437 - acc: 0.8367 - val_loss: 0.5530 - val_acc: 0.8352\n",
      "Epoch 132/200\n",
      "10s - loss: 0.5430 - acc: 0.8366 - val_loss: 0.5519 - val_acc: 0.8354\n",
      "Epoch 133/200\n",
      "10s - loss: 0.5409 - acc: 0.8372 - val_loss: 0.5498 - val_acc: 0.8369\n",
      "Epoch 134/200\n",
      "10s - loss: 0.5406 - acc: 0.8373 - val_loss: 0.5495 - val_acc: 0.8360\n",
      "Epoch 135/200\n",
      "10s - loss: 0.5397 - acc: 0.8379 - val_loss: 0.5507 - val_acc: 0.8363\n",
      "Epoch 136/200\n",
      "10s - loss: 0.5401 - acc: 0.8376 - val_loss: 0.5474 - val_acc: 0.8374\n",
      "Epoch 137/200\n",
      "10s - loss: 0.5396 - acc: 0.8377 - val_loss: 0.5490 - val_acc: 0.8365\n",
      "Epoch 138/200\n",
      "10s - loss: 0.5406 - acc: 0.8375 - val_loss: 0.5492 - val_acc: 0.8365\n",
      "Epoch 139/200\n",
      "10s - loss: 0.5385 - acc: 0.8381 - val_loss: 0.5538 - val_acc: 0.8350\n",
      "Epoch 140/200\n",
      "11s - loss: 0.5391 - acc: 0.8380 - val_loss: 0.5477 - val_acc: 0.8364\n",
      "Epoch 141/200\n",
      "10s - loss: 0.5383 - acc: 0.8381 - val_loss: 0.5513 - val_acc: 0.8351\n",
      "Epoch 142/200\n",
      "10s - loss: 0.5371 - acc: 0.8382 - val_loss: 0.5522 - val_acc: 0.8366\n",
      "Epoch 143/200\n",
      "11s - loss: 0.5365 - acc: 0.8387 - val_loss: 0.5456 - val_acc: 0.8376\n",
      "Epoch 144/200\n",
      "10s - loss: 0.5360 - acc: 0.8386 - val_loss: 0.5472 - val_acc: 0.8369\n",
      "Epoch 145/200\n",
      "10s - loss: 0.5373 - acc: 0.8384 - val_loss: 0.5504 - val_acc: 0.8366\n",
      "Epoch 146/200\n",
      "10s - loss: 0.5363 - acc: 0.8385 - val_loss: 0.5441 - val_acc: 0.8380\n",
      "Epoch 147/200\n",
      "10s - loss: 0.5351 - acc: 0.8388 - val_loss: 0.5425 - val_acc: 0.8382\n",
      "Epoch 148/200\n",
      "10s - loss: 0.5346 - acc: 0.8390 - val_loss: 0.5441 - val_acc: 0.8369\n",
      "Epoch 149/200\n",
      "10s - loss: 0.5345 - acc: 0.8392 - val_loss: 0.5436 - val_acc: 0.8384\n",
      "Epoch 150/200\n",
      "10s - loss: 0.5335 - acc: 0.8395 - val_loss: 0.5432 - val_acc: 0.8382\n",
      "Epoch 151/200\n",
      "10s - loss: 0.5347 - acc: 0.8389 - val_loss: 0.5419 - val_acc: 0.8389\n",
      "Epoch 152/200\n",
      "10s - loss: 0.5335 - acc: 0.8391 - val_loss: 0.5464 - val_acc: 0.8376\n",
      "Epoch 153/200\n",
      "10s - loss: 0.5328 - acc: 0.8396 - val_loss: 0.5436 - val_acc: 0.8377\n",
      "Epoch 154/200\n",
      "10s - loss: 0.5335 - acc: 0.8392 - val_loss: 0.5442 - val_acc: 0.8378\n",
      "Epoch 155/200\n",
      "10s - loss: 0.5316 - acc: 0.8400 - val_loss: 0.5424 - val_acc: 0.8385\n",
      "Epoch 156/200\n",
      "10s - loss: 0.5316 - acc: 0.8399 - val_loss: 0.5491 - val_acc: 0.8377\n",
      "Epoch 157/200\n",
      "10s - loss: 0.5320 - acc: 0.8398 - val_loss: 0.5444 - val_acc: 0.8382\n",
      "Epoch 158/200\n",
      "10s - loss: 0.5312 - acc: 0.8398 - val_loss: 0.5416 - val_acc: 0.8388\n",
      "Epoch 159/200\n",
      "10s - loss: 0.5315 - acc: 0.8400 - val_loss: 0.5431 - val_acc: 0.8381\n",
      "Epoch 160/200\n",
      "10s - loss: 0.5305 - acc: 0.8405 - val_loss: 0.5409 - val_acc: 0.8391\n",
      "Epoch 161/200\n",
      "10s - loss: 0.5295 - acc: 0.8406 - val_loss: 0.5459 - val_acc: 0.8375\n",
      "Epoch 162/200\n",
      "10s - loss: 0.5299 - acc: 0.8403 - val_loss: 0.5412 - val_acc: 0.8392\n",
      "Epoch 163/200\n",
      "10s - loss: 0.5295 - acc: 0.8403 - val_loss: 0.5443 - val_acc: 0.8384\n",
      "Epoch 164/200\n",
      "10s - loss: 0.5304 - acc: 0.8404 - val_loss: 0.5415 - val_acc: 0.8392\n",
      "Epoch 165/200\n",
      "10s - loss: 0.5293 - acc: 0.8408 - val_loss: 0.5447 - val_acc: 0.8384\n",
      "Epoch 166/200\n",
      "10s - loss: 0.5297 - acc: 0.8401 - val_loss: 0.5390 - val_acc: 0.8397\n",
      "Epoch 167/200\n",
      "10s - loss: 0.5277 - acc: 0.8410 - val_loss: 0.5386 - val_acc: 0.8393\n",
      "Epoch 168/200\n",
      "10s - loss: 0.5267 - acc: 0.8414 - val_loss: 0.5408 - val_acc: 0.8390\n",
      "Epoch 169/200\n",
      "10s - loss: 0.5270 - acc: 0.8411 - val_loss: 0.5391 - val_acc: 0.8399\n",
      "Epoch 170/200\n",
      "10s - loss: 0.5262 - acc: 0.8414 - val_loss: 0.5463 - val_acc: 0.8380\n",
      "Epoch 171/200\n",
      "10s - loss: 0.5288 - acc: 0.8406 - val_loss: 0.5448 - val_acc: 0.8386\n",
      "Epoch 172/200\n",
      "10s - loss: 0.5289 - acc: 0.8406 - val_loss: 0.5407 - val_acc: 0.8394\n",
      "Epoch 173/200\n",
      "10s - loss: 0.5264 - acc: 0.8412 - val_loss: 0.5374 - val_acc: 0.8395\n",
      "Epoch 174/200\n",
      "10s - loss: 0.5251 - acc: 0.8414 - val_loss: 0.5369 - val_acc: 0.8399\n",
      "Epoch 175/200\n",
      "10s - loss: 0.5249 - acc: 0.8416 - val_loss: 0.5326 - val_acc: 0.8414\n",
      "Epoch 176/200\n",
      "10s - loss: 0.5230 - acc: 0.8422 - val_loss: 0.5397 - val_acc: 0.8386\n",
      "Epoch 177/200\n",
      "10s - loss: 0.5257 - acc: 0.8413 - val_loss: 0.5369 - val_acc: 0.8400\n",
      "Epoch 178/200\n",
      "10s - loss: 0.5242 - acc: 0.8418 - val_loss: 0.5352 - val_acc: 0.8403\n",
      "Epoch 179/200\n",
      "10s - loss: 0.5244 - acc: 0.8419 - val_loss: 0.5391 - val_acc: 0.8396\n",
      "Epoch 180/200\n",
      "10s - loss: 0.5244 - acc: 0.8417 - val_loss: 0.5399 - val_acc: 0.8393\n",
      "Epoch 181/200\n",
      "10s - loss: 0.5230 - acc: 0.8419 - val_loss: 0.5318 - val_acc: 0.8418\n",
      "Epoch 182/200\n",
      "10s - loss: 0.5242 - acc: 0.8418 - val_loss: 0.5373 - val_acc: 0.8397\n",
      "Epoch 183/200\n",
      "10s - loss: 0.5227 - acc: 0.8423 - val_loss: 0.5373 - val_acc: 0.8395\n",
      "Epoch 184/200\n",
      "10s - loss: 0.5229 - acc: 0.8419 - val_loss: 0.5406 - val_acc: 0.8387\n",
      "Epoch 185/200\n",
      "10s - loss: 0.5238 - acc: 0.8417 - val_loss: 0.5333 - val_acc: 0.8402\n",
      "Epoch 186/200\n",
      "10s - loss: 0.5228 - acc: 0.8422 - val_loss: 0.5326 - val_acc: 0.8418\n",
      "Epoch 187/200\n",
      "10s - loss: 0.5213 - acc: 0.8423 - val_loss: 0.5412 - val_acc: 0.8390\n",
      "Epoch 188/200\n",
      "10s - loss: 0.5219 - acc: 0.8421 - val_loss: 0.5359 - val_acc: 0.8412\n",
      "Epoch 189/200\n",
      "10s - loss: 0.5227 - acc: 0.8419 - val_loss: 0.5332 - val_acc: 0.8411\n",
      "Epoch 190/200\n",
      "10s - loss: 0.5209 - acc: 0.8426 - val_loss: 0.5310 - val_acc: 0.8417\n",
      "Epoch 191/200\n",
      "10s - loss: 0.5204 - acc: 0.8427 - val_loss: 0.5300 - val_acc: 0.8416\n",
      "Epoch 192/200\n",
      "10s - loss: 0.5209 - acc: 0.8423 - val_loss: 0.5359 - val_acc: 0.8407\n",
      "Epoch 193/200\n",
      "10s - loss: 0.5201 - acc: 0.8428 - val_loss: 0.5318 - val_acc: 0.8419\n",
      "Epoch 194/200\n",
      "10s - loss: 0.5186 - acc: 0.8431 - val_loss: 0.5332 - val_acc: 0.8408\n",
      "Epoch 195/200\n",
      "10s - loss: 0.5198 - acc: 0.8426 - val_loss: 0.5318 - val_acc: 0.8413\n",
      "Epoch 196/200\n",
      "10s - loss: 0.5203 - acc: 0.8426 - val_loss: 0.5318 - val_acc: 0.8414\n",
      "Epoch 197/200\n",
      "10s - loss: 0.5189 - acc: 0.8431 - val_loss: 0.5322 - val_acc: 0.8420\n",
      "Epoch 198/200\n",
      "10s - loss: 0.5196 - acc: 0.8428 - val_loss: 0.5299 - val_acc: 0.8416\n",
      "Epoch 199/200\n",
      "10s - loss: 0.5180 - acc: 0.8433 - val_loss: 0.5351 - val_acc: 0.8409\n",
      "Epoch 200/200\n",
      "10s - loss: 0.5188 - acc: 0.8433 - val_loss: 0.5307 - val_acc: 0.8415\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nu8cvkj37jbgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2511db08710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "new jersey est parfois calme en l' et il est est neigeux en en en en en en en en en\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "history = History()                 #need this in order to do my vsualizations\n",
    "\n",
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a RNN model using word embedding on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement -- Done!\n",
    "    \"\"\"\n",
    "    Keras components/tools ...\n",
    "    Embedding        -- Turns positive integers (indexes) into dense vectors of fixed size. \n",
    "                        eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
    "                        mask_zero: Whether or not the input value 0 is a special \n",
    "                        \"padding\" value that should be masked out.              \n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(english_vocab_size, french_vocab_size, input_length=output_sequence_length, mask_zero=True))\n",
    "    model.add(GRU(output_sequence_length, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size)))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tests.test_embed_model(embed_model)\n",
    "\n",
    "\n",
    "# TODO: Reshape the input -- Done!\n",
    "# borrowed from simple_model\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
    "\n",
    "\n",
    "# TODO: Train the neural network -- Done!\n",
    "# borrowed from simple_model\n",
    "embed_rnn_model = embed_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1)\n",
    "embed_rnn_model.summary()\n",
    "embed_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=200, validation_split=0.2, verbose=2, callbacks=[history])\n",
    "\n",
    "# Add visualization\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('embedding model loss - 200 epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train loss', 'validation loss'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# TODO: Print prediction(s) -- Done!\n",
    "# borrowed from simple_model\n",
    "print(english_sentences[0])\n",
    "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "\n",
    "#batch = 64     => 91s/batch Epoch 5/5 -91s - loss: 0.7616 - acc: 0.7788 - val_loss: 0.7380 - val_acc: 0.7846\n",
    "#batch = 1024   => 91s/batch Epoch 5/5 -10s - loss: 1.9122 - acc: 0.5031 - val_loss: 1.8141 - val_acc: 0.5142"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![RNN](embedding model loss - 5 epochs.png)![RNN](embedding model loss - 200 epochs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model 3: Bidirectional RNNs (IMPLEMENTATION)\n",
    "![RNN](images/bidirectional.png)\n",
    "One restriction of a RNN is that it can't see the future input, only the past.  This is where bidirectional recurrent neural networks come in.  They are able to see the future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_2 (Bidirection (None, 21, 42)            2898      \n",
      "_________________________________________________________________\n",
      "time_distributed_32 (TimeDis (None, 21, 346)           14878     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 21, 346)           0         \n",
      "=================================================================\n",
      "Total params: 17,776\n",
      "Trainable params: 17,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/200\n",
      "9s - loss: 4.3502 - acc: 0.4055 - val_loss: 2.8159 - val_acc: 0.4633\n",
      "Epoch 2/200\n",
      "7s - loss: 2.6273 - acc: 0.4780 - val_loss: 2.4867 - val_acc: 0.4832\n",
      "Epoch 3/200\n",
      "7s - loss: 2.3742 - acc: 0.4922 - val_loss: 2.2604 - val_acc: 0.5049\n",
      "Epoch 4/200\n",
      "7s - loss: 2.1584 - acc: 0.5241 - val_loss: 2.0504 - val_acc: 0.5493\n",
      "Epoch 5/200\n",
      "7s - loss: 1.9520 - acc: 0.5598 - val_loss: 1.8606 - val_acc: 0.5694\n",
      "Epoch 6/200\n",
      "7s - loss: 1.7957 - acc: 0.5725 - val_loss: 1.7398 - val_acc: 0.5778\n",
      "Epoch 7/200\n",
      "7s - loss: 1.7000 - acc: 0.5809 - val_loss: 1.6655 - val_acc: 0.5843\n",
      "Epoch 8/200\n",
      "7s - loss: 1.6388 - acc: 0.5840 - val_loss: 1.6166 - val_acc: 0.5859\n",
      "Epoch 9/200\n",
      "7s - loss: 1.5966 - acc: 0.5901 - val_loss: 1.5794 - val_acc: 0.5958\n",
      "Epoch 10/200\n",
      "7s - loss: 1.5634 - acc: 0.5998 - val_loss: 1.5491 - val_acc: 0.6048\n",
      "Epoch 11/200\n",
      "7s - loss: 1.5347 - acc: 0.6058 - val_loss: 1.5218 - val_acc: 0.6083\n",
      "Epoch 12/200\n",
      "7s - loss: 1.5091 - acc: 0.6100 - val_loss: 1.4978 - val_acc: 0.6122\n",
      "Epoch 13/200\n",
      "7s - loss: 1.4865 - acc: 0.6141 - val_loss: 1.4763 - val_acc: 0.6186\n",
      "Epoch 14/200\n",
      "7s - loss: 1.4659 - acc: 0.6188 - val_loss: 1.4569 - val_acc: 0.6201\n",
      "Epoch 15/200\n",
      "7s - loss: 1.4470 - acc: 0.6214 - val_loss: 1.4385 - val_acc: 0.6235\n",
      "Epoch 16/200\n",
      "7s - loss: 1.4295 - acc: 0.6238 - val_loss: 1.4217 - val_acc: 0.6260\n",
      "Epoch 17/200\n",
      "7s - loss: 1.4137 - acc: 0.6253 - val_loss: 1.4065 - val_acc: 0.6271\n",
      "Epoch 18/200\n",
      "7s - loss: 1.3988 - acc: 0.6274 - val_loss: 1.3921 - val_acc: 0.6295\n",
      "Epoch 19/200\n",
      "7s - loss: 1.3845 - acc: 0.6297 - val_loss: 1.3780 - val_acc: 0.6334\n",
      "Epoch 20/200\n",
      "7s - loss: 1.3706 - acc: 0.6320 - val_loss: 1.3644 - val_acc: 0.6310\n",
      "Epoch 21/200\n",
      "7s - loss: 1.3572 - acc: 0.6334 - val_loss: 1.3512 - val_acc: 0.6332\n",
      "Epoch 22/200\n",
      "7s - loss: 1.3444 - acc: 0.6347 - val_loss: 1.3388 - val_acc: 0.6362\n",
      "Epoch 23/200\n",
      "7s - loss: 1.3323 - acc: 0.6367 - val_loss: 1.3270 - val_acc: 0.6391\n",
      "Epoch 24/200\n",
      "7s - loss: 1.3206 - acc: 0.6387 - val_loss: 1.3156 - val_acc: 0.6396\n",
      "Epoch 25/200\n",
      "7s - loss: 1.3095 - acc: 0.6417 - val_loss: 1.3048 - val_acc: 0.6421\n",
      "Epoch 26/200\n",
      "7s - loss: 1.2987 - acc: 0.6440 - val_loss: 1.2942 - val_acc: 0.6452\n",
      "Epoch 27/200\n",
      "7s - loss: 1.2882 - acc: 0.6459 - val_loss: 1.2837 - val_acc: 0.6477\n",
      "Epoch 28/200\n",
      "7s - loss: 1.2779 - acc: 0.6477 - val_loss: 1.2737 - val_acc: 0.6497\n",
      "Epoch 29/200\n",
      "7s - loss: 1.2682 - acc: 0.6494 - val_loss: 1.2640 - val_acc: 0.6500\n",
      "Epoch 30/200\n",
      "7s - loss: 1.2589 - acc: 0.6505 - val_loss: 1.2553 - val_acc: 0.6519\n",
      "Epoch 31/200\n",
      "7s - loss: 1.2504 - acc: 0.6521 - val_loss: 1.2469 - val_acc: 0.6528\n",
      "Epoch 32/200\n",
      "7s - loss: 1.2423 - acc: 0.6532 - val_loss: 1.2392 - val_acc: 0.6543\n",
      "Epoch 33/200\n",
      "7s - loss: 1.2349 - acc: 0.6545 - val_loss: 1.2316 - val_acc: 0.6543\n",
      "Epoch 34/200\n",
      "7s - loss: 1.2278 - acc: 0.6554 - val_loss: 1.2248 - val_acc: 0.6557\n",
      "Epoch 35/200\n",
      "7s - loss: 1.2212 - acc: 0.6563 - val_loss: 1.2184 - val_acc: 0.6552\n",
      "Epoch 36/200\n",
      "7s - loss: 1.2151 - acc: 0.6570 - val_loss: 1.2125 - val_acc: 0.6582\n",
      "Epoch 37/200\n",
      "7s - loss: 1.2093 - acc: 0.6577 - val_loss: 1.2069 - val_acc: 0.6590\n",
      "Epoch 38/200\n",
      "7s - loss: 1.2038 - acc: 0.6586 - val_loss: 1.2016 - val_acc: 0.6586\n",
      "Epoch 39/200\n",
      "7s - loss: 1.1985 - acc: 0.6593 - val_loss: 1.1968 - val_acc: 0.6594\n",
      "Epoch 40/200\n",
      "7s - loss: 1.1936 - acc: 0.6600 - val_loss: 1.1915 - val_acc: 0.6596\n",
      "Epoch 41/200\n",
      "7s - loss: 1.1888 - acc: 0.6602 - val_loss: 1.1867 - val_acc: 0.6611\n",
      "Epoch 42/200\n",
      "7s - loss: 1.1843 - acc: 0.6610 - val_loss: 1.1825 - val_acc: 0.6614\n",
      "Epoch 43/200\n",
      "7s - loss: 1.1801 - acc: 0.6616 - val_loss: 1.1781 - val_acc: 0.6623\n",
      "Epoch 44/200\n",
      "7s - loss: 1.1760 - acc: 0.6620 - val_loss: 1.1743 - val_acc: 0.6631\n",
      "Epoch 45/200\n",
      "7s - loss: 1.1721 - acc: 0.6626 - val_loss: 1.1702 - val_acc: 0.6633\n",
      "Epoch 46/200\n",
      "7s - loss: 1.1683 - acc: 0.6634 - val_loss: 1.1669 - val_acc: 0.6635\n",
      "Epoch 47/200\n",
      "7s - loss: 1.1647 - acc: 0.6637 - val_loss: 1.1632 - val_acc: 0.6642\n",
      "Epoch 48/200\n",
      "7s - loss: 1.1612 - acc: 0.6644 - val_loss: 1.1597 - val_acc: 0.6652\n",
      "Epoch 49/200\n",
      "7s - loss: 1.1579 - acc: 0.6643 - val_loss: 1.1568 - val_acc: 0.6649\n",
      "Epoch 50/200\n",
      "7s - loss: 1.1547 - acc: 0.6651 - val_loss: 1.1534 - val_acc: 0.6653\n",
      "Epoch 51/200\n",
      "7s - loss: 1.1515 - acc: 0.6652 - val_loss: 1.1501 - val_acc: 0.6660\n",
      "Epoch 52/200\n",
      "7s - loss: 1.1484 - acc: 0.6656 - val_loss: 1.1471 - val_acc: 0.6667\n",
      "Epoch 53/200\n",
      "7s - loss: 1.1455 - acc: 0.6660 - val_loss: 1.1441 - val_acc: 0.6666\n",
      "Epoch 54/200\n",
      "7s - loss: 1.1426 - acc: 0.6662 - val_loss: 1.1419 - val_acc: 0.6660\n",
      "Epoch 55/200\n",
      "7s - loss: 1.1399 - acc: 0.6666 - val_loss: 1.1389 - val_acc: 0.6664\n",
      "Epoch 56/200\n",
      "7s - loss: 1.1374 - acc: 0.6668 - val_loss: 1.1362 - val_acc: 0.6671\n",
      "Epoch 57/200\n",
      "7s - loss: 1.1347 - acc: 0.6673 - val_loss: 1.1338 - val_acc: 0.6680\n",
      "Epoch 58/200\n",
      "7s - loss: 1.1324 - acc: 0.6675 - val_loss: 1.1315 - val_acc: 0.6669\n",
      "Epoch 59/200\n",
      "7s - loss: 1.1298 - acc: 0.6678 - val_loss: 1.1290 - val_acc: 0.6679\n",
      "Epoch 60/200\n",
      "7s - loss: 1.1275 - acc: 0.6677 - val_loss: 1.1266 - val_acc: 0.6682\n",
      "Epoch 61/200\n",
      "7s - loss: 1.1252 - acc: 0.6682 - val_loss: 1.1243 - val_acc: 0.6670\n",
      "Epoch 62/200\n",
      "7s - loss: 1.1229 - acc: 0.6687 - val_loss: 1.1220 - val_acc: 0.6687\n",
      "Epoch 63/200\n",
      "7s - loss: 1.1209 - acc: 0.6689 - val_loss: 1.1203 - val_acc: 0.6691\n",
      "Epoch 64/200\n",
      "7s - loss: 1.1188 - acc: 0.6690 - val_loss: 1.1180 - val_acc: 0.6691\n",
      "Epoch 65/200\n",
      "7s - loss: 1.1167 - acc: 0.6693 - val_loss: 1.1160 - val_acc: 0.6698\n",
      "Epoch 66/200\n",
      "7s - loss: 1.1146 - acc: 0.6699 - val_loss: 1.1139 - val_acc: 0.6715\n",
      "Epoch 67/200\n",
      "7s - loss: 1.1126 - acc: 0.6704 - val_loss: 1.1118 - val_acc: 0.6706\n",
      "Epoch 68/200\n",
      "7s - loss: 1.1107 - acc: 0.6706 - val_loss: 1.1102 - val_acc: 0.6694\n",
      "Epoch 69/200\n",
      "7s - loss: 1.1088 - acc: 0.6711 - val_loss: 1.1083 - val_acc: 0.6706\n",
      "Epoch 70/200\n",
      "7s - loss: 1.1070 - acc: 0.6716 - val_loss: 1.1060 - val_acc: 0.6719\n",
      "Epoch 71/200\n",
      "7s - loss: 1.1049 - acc: 0.6720 - val_loss: 1.1043 - val_acc: 0.6720\n",
      "Epoch 72/200\n",
      "7s - loss: 1.1031 - acc: 0.6724 - val_loss: 1.1027 - val_acc: 0.6726\n",
      "Epoch 73/200\n",
      "7s - loss: 1.1014 - acc: 0.6731 - val_loss: 1.1011 - val_acc: 0.6733\n",
      "Epoch 74/200\n",
      "7s - loss: 1.0995 - acc: 0.6734 - val_loss: 1.0989 - val_acc: 0.6730\n",
      "Epoch 75/200\n",
      "7s - loss: 1.0978 - acc: 0.6741 - val_loss: 1.0975 - val_acc: 0.6742\n",
      "Epoch 76/200\n",
      "7s - loss: 1.0961 - acc: 0.6744 - val_loss: 1.0955 - val_acc: 0.6746\n",
      "Epoch 77/200\n",
      "7s - loss: 1.0945 - acc: 0.6746 - val_loss: 1.0938 - val_acc: 0.6738\n",
      "Epoch 78/200\n",
      "7s - loss: 1.0929 - acc: 0.6750 - val_loss: 1.0924 - val_acc: 0.6748\n",
      "Epoch 79/200\n",
      "7s - loss: 1.0913 - acc: 0.6753 - val_loss: 1.0909 - val_acc: 0.6750\n",
      "Epoch 80/200\n",
      "7s - loss: 1.0898 - acc: 0.6759 - val_loss: 1.0895 - val_acc: 0.6759\n",
      "Epoch 81/200\n",
      "7s - loss: 1.0883 - acc: 0.6759 - val_loss: 1.0880 - val_acc: 0.6745\n",
      "Epoch 82/200\n",
      "7s - loss: 1.0868 - acc: 0.6764 - val_loss: 1.0864 - val_acc: 0.6760\n",
      "Epoch 83/200\n",
      "7s - loss: 1.0854 - acc: 0.6764 - val_loss: 1.0849 - val_acc: 0.6764\n",
      "Epoch 84/200\n",
      "7s - loss: 1.0837 - acc: 0.6770 - val_loss: 1.0833 - val_acc: 0.6767\n",
      "Epoch 85/200\n",
      "7s - loss: 1.0824 - acc: 0.6773 - val_loss: 1.0822 - val_acc: 0.6764\n",
      "Epoch 86/200\n",
      "7s - loss: 1.0810 - acc: 0.6776 - val_loss: 1.0806 - val_acc: 0.6775\n",
      "Epoch 87/200\n",
      "7s - loss: 1.0797 - acc: 0.6776 - val_loss: 1.0794 - val_acc: 0.6775\n",
      "Epoch 88/200\n",
      "7s - loss: 1.0783 - acc: 0.6781 - val_loss: 1.0779 - val_acc: 0.6779\n",
      "Epoch 89/200\n",
      "7s - loss: 1.0769 - acc: 0.6784 - val_loss: 1.0767 - val_acc: 0.6775\n",
      "Epoch 90/200\n",
      "7s - loss: 1.0757 - acc: 0.6784 - val_loss: 1.0751 - val_acc: 0.6780\n",
      "Epoch 91/200\n",
      "7s - loss: 1.0741 - acc: 0.6786 - val_loss: 1.0735 - val_acc: 0.6790\n",
      "Epoch 92/200\n",
      "7s - loss: 1.0729 - acc: 0.6790 - val_loss: 1.0725 - val_acc: 0.6786\n",
      "Epoch 93/200\n",
      "7s - loss: 1.0714 - acc: 0.6791 - val_loss: 1.0710 - val_acc: 0.6795\n",
      "Epoch 94/200\n",
      "7s - loss: 1.0701 - acc: 0.6796 - val_loss: 1.0698 - val_acc: 0.6802\n",
      "Epoch 95/200\n",
      "7s - loss: 1.0689 - acc: 0.6800 - val_loss: 1.0687 - val_acc: 0.6796\n",
      "Epoch 96/200\n",
      "7s - loss: 1.0678 - acc: 0.6801 - val_loss: 1.0677 - val_acc: 0.6801\n",
      "Epoch 97/200\n",
      "7s - loss: 1.0664 - acc: 0.6806 - val_loss: 1.0660 - val_acc: 0.6814\n",
      "Epoch 98/200\n",
      "7s - loss: 1.0652 - acc: 0.6810 - val_loss: 1.0647 - val_acc: 0.6810\n",
      "Epoch 99/200\n",
      "7s - loss: 1.0642 - acc: 0.6812 - val_loss: 1.0638 - val_acc: 0.6796\n",
      "Epoch 100/200\n",
      "7s - loss: 1.0629 - acc: 0.6817 - val_loss: 1.0626 - val_acc: 0.6814\n",
      "Epoch 101/200\n",
      "7s - loss: 1.0617 - acc: 0.6821 - val_loss: 1.0612 - val_acc: 0.6817\n",
      "Epoch 102/200\n",
      "7s - loss: 1.0604 - acc: 0.6830 - val_loss: 1.0599 - val_acc: 0.6842\n",
      "Epoch 103/200\n",
      "7s - loss: 1.0592 - acc: 0.6837 - val_loss: 1.0588 - val_acc: 0.6845\n",
      "Epoch 104/200\n",
      "7s - loss: 1.0580 - acc: 0.6843 - val_loss: 1.0576 - val_acc: 0.6832\n",
      "Epoch 105/200\n",
      "7s - loss: 1.0568 - acc: 0.6847 - val_loss: 1.0567 - val_acc: 0.6860\n",
      "Epoch 106/200\n",
      "7s - loss: 1.0556 - acc: 0.6852 - val_loss: 1.0554 - val_acc: 0.6856\n",
      "Epoch 107/200\n",
      "7s - loss: 1.0544 - acc: 0.6854 - val_loss: 1.0541 - val_acc: 0.6855\n",
      "Epoch 108/200\n",
      "7s - loss: 1.0532 - acc: 0.6857 - val_loss: 1.0525 - val_acc: 0.6861\n",
      "Epoch 109/200\n",
      "7s - loss: 1.0520 - acc: 0.6862 - val_loss: 1.0517 - val_acc: 0.6857\n",
      "Epoch 110/200\n",
      "7s - loss: 1.0507 - acc: 0.6865 - val_loss: 1.0503 - val_acc: 0.6874\n",
      "Epoch 111/200\n",
      "7s - loss: 1.0496 - acc: 0.6871 - val_loss: 1.0493 - val_acc: 0.6861\n",
      "Epoch 112/200\n",
      "7s - loss: 1.0482 - acc: 0.6868 - val_loss: 1.0482 - val_acc: 0.6863\n",
      "Epoch 113/200\n",
      "7s - loss: 1.0471 - acc: 0.6872 - val_loss: 1.0473 - val_acc: 0.6869\n",
      "Epoch 114/200\n",
      "7s - loss: 1.0459 - acc: 0.6875 - val_loss: 1.0460 - val_acc: 0.6880\n",
      "Epoch 115/200\n",
      "7s - loss: 1.0449 - acc: 0.6879 - val_loss: 1.0452 - val_acc: 0.6874\n",
      "Epoch 116/200\n",
      "7s - loss: 1.0435 - acc: 0.6881 - val_loss: 1.0435 - val_acc: 0.6881\n",
      "Epoch 117/200\n",
      "7s - loss: 1.0422 - acc: 0.6881 - val_loss: 1.0423 - val_acc: 0.6866\n",
      "Epoch 118/200\n",
      "7s - loss: 1.0411 - acc: 0.6884 - val_loss: 1.0408 - val_acc: 0.6884\n",
      "Epoch 119/200\n",
      "7s - loss: 1.0397 - acc: 0.6893 - val_loss: 1.0398 - val_acc: 0.6885\n",
      "Epoch 120/200\n",
      "7s - loss: 1.0386 - acc: 0.6892 - val_loss: 1.0383 - val_acc: 0.6877\n",
      "Epoch 121/200\n",
      "7s - loss: 1.0371 - acc: 0.6900 - val_loss: 1.0372 - val_acc: 0.6890\n",
      "Epoch 122/200\n",
      "7s - loss: 1.0358 - acc: 0.6904 - val_loss: 1.0357 - val_acc: 0.6899\n",
      "Epoch 123/200\n",
      "7s - loss: 1.0345 - acc: 0.6909 - val_loss: 1.0342 - val_acc: 0.6906\n",
      "Epoch 124/200\n",
      "7s - loss: 1.0335 - acc: 0.6910 - val_loss: 1.0327 - val_acc: 0.6920\n",
      "Epoch 125/200\n",
      "7s - loss: 1.0320 - acc: 0.6918 - val_loss: 1.0323 - val_acc: 0.6916\n",
      "Epoch 126/200\n",
      "7s - loss: 1.0308 - acc: 0.6920 - val_loss: 1.0309 - val_acc: 0.6928\n",
      "Epoch 127/200\n",
      "7s - loss: 1.0295 - acc: 0.6923 - val_loss: 1.0293 - val_acc: 0.6931\n",
      "Epoch 128/200\n",
      "7s - loss: 1.0283 - acc: 0.6930 - val_loss: 1.0281 - val_acc: 0.6943\n",
      "Epoch 129/200\n",
      "7s - loss: 1.0273 - acc: 0.6931 - val_loss: 1.0278 - val_acc: 0.6934\n",
      "Epoch 130/200\n",
      "7s - loss: 1.0265 - acc: 0.6933 - val_loss: 1.0267 - val_acc: 0.6924\n",
      "Epoch 131/200\n",
      "7s - loss: 1.0251 - acc: 0.6936 - val_loss: 1.0247 - val_acc: 0.6940\n",
      "Epoch 132/200\n",
      "7s - loss: 1.0236 - acc: 0.6936 - val_loss: 1.0237 - val_acc: 0.6930\n",
      "Epoch 133/200\n",
      "7s - loss: 1.0225 - acc: 0.6942 - val_loss: 1.0221 - val_acc: 0.6948\n",
      "Epoch 134/200\n",
      "7s - loss: 1.0213 - acc: 0.6946 - val_loss: 1.0219 - val_acc: 0.6946\n",
      "Epoch 135/200\n",
      "7s - loss: 1.0203 - acc: 0.6946 - val_loss: 1.0206 - val_acc: 0.6949\n",
      "Epoch 136/200\n",
      "7s - loss: 1.0191 - acc: 0.6946 - val_loss: 1.0196 - val_acc: 0.6940\n",
      "Epoch 137/200\n",
      "7s - loss: 1.0179 - acc: 0.6953 - val_loss: 1.0176 - val_acc: 0.6943\n",
      "Epoch 138/200\n",
      "7s - loss: 1.0165 - acc: 0.6952 - val_loss: 1.0171 - val_acc: 0.6955\n",
      "Epoch 139/200\n",
      "7s - loss: 1.0155 - acc: 0.6957 - val_loss: 1.0156 - val_acc: 0.6967\n",
      "Epoch 140/200\n",
      "7s - loss: 1.0143 - acc: 0.6960 - val_loss: 1.0142 - val_acc: 0.6946\n",
      "Epoch 141/200\n",
      "7s - loss: 1.0131 - acc: 0.6957 - val_loss: 1.0137 - val_acc: 0.6961\n",
      "Epoch 142/200\n",
      "7s - loss: 1.0119 - acc: 0.6962 - val_loss: 1.0120 - val_acc: 0.6968\n",
      "Epoch 143/200\n",
      "7s - loss: 1.0106 - acc: 0.6965 - val_loss: 1.0104 - val_acc: 0.6965\n",
      "Epoch 144/200\n",
      "7s - loss: 1.0093 - acc: 0.6966 - val_loss: 1.0095 - val_acc: 0.6961\n",
      "Epoch 145/200\n",
      "7s - loss: 1.0080 - acc: 0.6968 - val_loss: 1.0080 - val_acc: 0.6971\n",
      "Epoch 146/200\n",
      "7s - loss: 1.0071 - acc: 0.6971 - val_loss: 1.0073 - val_acc: 0.6978\n",
      "Epoch 147/200\n",
      "7s - loss: 1.0056 - acc: 0.6973 - val_loss: 1.0060 - val_acc: 0.6973\n",
      "Epoch 148/200\n",
      "7s - loss: 1.0043 - acc: 0.6975 - val_loss: 1.0049 - val_acc: 0.6978\n",
      "Epoch 149/200\n",
      "7s - loss: 1.0032 - acc: 0.6975 - val_loss: 1.0036 - val_acc: 0.6972\n",
      "Epoch 150/200\n",
      "7s - loss: 1.0020 - acc: 0.6979 - val_loss: 1.0021 - val_acc: 0.6983\n",
      "Epoch 151/200\n",
      "7s - loss: 1.0008 - acc: 0.6978 - val_loss: 1.0011 - val_acc: 0.6969\n",
      "Epoch 152/200\n",
      "7s - loss: 0.9995 - acc: 0.6981 - val_loss: 0.9998 - val_acc: 0.6980\n",
      "Epoch 153/200\n",
      "7s - loss: 0.9985 - acc: 0.6981 - val_loss: 0.9985 - val_acc: 0.6975\n",
      "Epoch 154/200\n",
      "7s - loss: 0.9972 - acc: 0.6986 - val_loss: 0.9984 - val_acc: 0.6991\n",
      "Epoch 155/200\n",
      "7s - loss: 0.9962 - acc: 0.6989 - val_loss: 0.9964 - val_acc: 0.6993\n",
      "Epoch 156/200\n",
      "7s - loss: 0.9949 - acc: 0.6992 - val_loss: 0.9954 - val_acc: 0.6986\n",
      "Epoch 157/200\n",
      "7s - loss: 0.9936 - acc: 0.6994 - val_loss: 0.9945 - val_acc: 0.7000\n",
      "Epoch 158/200\n",
      "7s - loss: 0.9924 - acc: 0.6995 - val_loss: 0.9931 - val_acc: 0.6993\n",
      "Epoch 159/200\n",
      "7s - loss: 0.9915 - acc: 0.6997 - val_loss: 0.9914 - val_acc: 0.7000\n",
      "Epoch 160/200\n",
      "7s - loss: 0.9900 - acc: 0.7000 - val_loss: 0.9902 - val_acc: 0.7002\n",
      "Epoch 161/200\n",
      "7s - loss: 0.9893 - acc: 0.7002 - val_loss: 0.9900 - val_acc: 0.6998\n",
      "Epoch 162/200\n",
      "7s - loss: 0.9878 - acc: 0.7006 - val_loss: 0.9881 - val_acc: 0.7015\n",
      "Epoch 163/200\n",
      "7s - loss: 0.9866 - acc: 0.7010 - val_loss: 0.9873 - val_acc: 0.6990\n",
      "Epoch 164/200\n",
      "7s - loss: 0.9858 - acc: 0.7010 - val_loss: 0.9865 - val_acc: 0.7009\n",
      "Epoch 165/200\n",
      "7s - loss: 0.9847 - acc: 0.7017 - val_loss: 0.9853 - val_acc: 0.7027\n",
      "Epoch 166/200\n",
      "7s - loss: 0.9836 - acc: 0.7018 - val_loss: 0.9839 - val_acc: 0.7013\n",
      "Epoch 167/200\n",
      "7s - loss: 0.9824 - acc: 0.7022 - val_loss: 0.9829 - val_acc: 0.7016\n",
      "Epoch 168/200\n",
      "7s - loss: 0.9814 - acc: 0.7025 - val_loss: 0.9824 - val_acc: 0.7026\n",
      "Epoch 169/200\n",
      "7s - loss: 0.9803 - acc: 0.7031 - val_loss: 0.9815 - val_acc: 0.7025\n",
      "Epoch 170/200\n",
      "7s - loss: 0.9792 - acc: 0.7033 - val_loss: 0.9802 - val_acc: 0.7034\n",
      "Epoch 171/200\n",
      "7s - loss: 0.9781 - acc: 0.7038 - val_loss: 0.9785 - val_acc: 0.7039\n",
      "Epoch 172/200\n",
      "7s - loss: 0.9771 - acc: 0.7040 - val_loss: 0.9778 - val_acc: 0.7048\n",
      "Epoch 173/200\n",
      "7s - loss: 0.9760 - acc: 0.7045 - val_loss: 0.9770 - val_acc: 0.7037\n",
      "Epoch 174/200\n",
      "7s - loss: 0.9749 - acc: 0.7050 - val_loss: 0.9766 - val_acc: 0.7036\n",
      "Epoch 175/200\n",
      "7s - loss: 0.9738 - acc: 0.7053 - val_loss: 0.9747 - val_acc: 0.7052\n",
      "Epoch 176/200\n",
      "7s - loss: 0.9727 - acc: 0.7056 - val_loss: 0.9733 - val_acc: 0.7054\n",
      "Epoch 177/200\n",
      "7s - loss: 0.9718 - acc: 0.7057 - val_loss: 0.9729 - val_acc: 0.7054\n",
      "Epoch 178/200\n",
      "7s - loss: 0.9706 - acc: 0.7062 - val_loss: 0.9713 - val_acc: 0.7069\n",
      "Epoch 179/200\n",
      "7s - loss: 0.9696 - acc: 0.7063 - val_loss: 0.9705 - val_acc: 0.7056\n",
      "Epoch 180/200\n",
      "7s - loss: 0.9684 - acc: 0.7069 - val_loss: 0.9694 - val_acc: 0.7063\n",
      "Epoch 181/200\n",
      "7s - loss: 0.9674 - acc: 0.7072 - val_loss: 0.9682 - val_acc: 0.7067\n",
      "Epoch 182/200\n",
      "7s - loss: 0.9667 - acc: 0.7073 - val_loss: 0.9671 - val_acc: 0.7065\n",
      "Epoch 183/200\n",
      "7s - loss: 0.9654 - acc: 0.7078 - val_loss: 0.9666 - val_acc: 0.7076\n",
      "Epoch 184/200\n",
      "7s - loss: 0.9644 - acc: 0.7081 - val_loss: 0.9659 - val_acc: 0.7072\n",
      "Epoch 185/200\n",
      "7s - loss: 0.9635 - acc: 0.7082 - val_loss: 0.9640 - val_acc: 0.7089\n",
      "Epoch 186/200\n",
      "7s - loss: 0.9624 - acc: 0.7088 - val_loss: 0.9632 - val_acc: 0.7080\n",
      "Epoch 187/200\n",
      "7s - loss: 0.9613 - acc: 0.7089 - val_loss: 0.9623 - val_acc: 0.7088\n",
      "Epoch 188/200\n",
      "7s - loss: 0.9607 - acc: 0.7090 - val_loss: 0.9614 - val_acc: 0.7083\n",
      "Epoch 189/200\n",
      "7s - loss: 0.9597 - acc: 0.7095 - val_loss: 0.9610 - val_acc: 0.7099\n",
      "Epoch 190/200\n",
      "7s - loss: 0.9587 - acc: 0.7095 - val_loss: 0.9599 - val_acc: 0.7097\n",
      "Epoch 191/200\n",
      "7s - loss: 0.9579 - acc: 0.7100 - val_loss: 0.9586 - val_acc: 0.7097\n",
      "Epoch 192/200\n",
      "7s - loss: 0.9569 - acc: 0.7103 - val_loss: 0.9576 - val_acc: 0.7098\n",
      "Epoch 193/200\n",
      "7s - loss: 0.9560 - acc: 0.7106 - val_loss: 0.9568 - val_acc: 0.7099\n",
      "Epoch 194/200\n",
      "7s - loss: 0.9552 - acc: 0.7108 - val_loss: 0.9562 - val_acc: 0.7111\n",
      "Epoch 195/200\n",
      "7s - loss: 0.9543 - acc: 0.7111 - val_loss: 0.9551 - val_acc: 0.7106\n",
      "Epoch 196/200\n",
      "7s - loss: 0.9535 - acc: 0.7112 - val_loss: 0.9540 - val_acc: 0.7107\n",
      "Epoch 197/200\n",
      "7s - loss: 0.9528 - acc: 0.7112 - val_loss: 0.9538 - val_acc: 0.7117\n",
      "Epoch 198/200\n",
      "7s - loss: 0.9518 - acc: 0.7116 - val_loss: 0.9525 - val_acc: 0.7115\n",
      "Epoch 199/200\n",
      "7s - loss: 0.9509 - acc: 0.7122 - val_loss: 0.9517 - val_acc: 0.7115\n",
      "Epoch 200/200\n",
      "7s - loss: 0.9499 - acc: 0.7127 - val_loss: 0.9517 - val_acc: 0.7114\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUhARkV5KCiIi0uv/ARcN72UtenEXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25126664d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "new jersey est parfois humide en mois de il il il en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "history = History()                 #need this in order to do my vsualizations\n",
    "\n",
    "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a bidirectional RNN model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement -- Done!\n",
    "    \"\"\"\n",
    "    Keras components/tools ...\n",
    "    Biderctional - Bidirectional wrapper for RNNs. Can be applied tto LSTMs and GRUs                   \n",
    "    \"\"\"   \n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(GRU(output_sequence_length, return_sequences=True), input_shape=input_shape[1:]))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size)))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "tests.test_bd_model(bd_model)\n",
    "\n",
    "\n",
    "# TODO: Train and Print prediction(s) -- Done!\n",
    "# borrowed from simple_model\n",
    "# Reshaping the input\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "bd_rnn_model = bd_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1)\n",
    "bd_rnn_model.summary()\n",
    "bd_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=200, validation_split=0.2, verbose=2, callbacks=[history])\n",
    "\n",
    "\n",
    "# Add visualization\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('bidirectional model loss - 200 epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train loss', 'validation loss'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "print(english_sentences[0])\n",
    "print(logits_to_text(bd_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![RNN](bidirectional model loss - 5 epochs.png)![RNN](bidirectional model loss - 200 epochs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model 4: Encoder-Decoder (OPTIONAL)\n",
    "Time to look at encoder-decoder models.  This model is made up of an encoder and decoder. The encoder creates a matrix representation of the sentence.  The decoder takes this matrix as input and predicts the translation as output.\n",
    "\n",
    "Create an encoder-decoder model in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_35 (GRU)                 (None, 21)                1449      \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 21, 21)            0         \n",
      "_________________________________________________________________\n",
      "gru_36 (GRU)                 (None, 21, 21)            2709      \n",
      "_________________________________________________________________\n",
      "time_distributed_34 (TimeDis (None, 21, 346)           7612      \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 21, 346)           0         \n",
      "=================================================================\n",
      "Total params: 11,770\n",
      "Trainable params: 11,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/200\n",
      "10s - loss: 4.4925 - acc: 0.2628 - val_loss: 3.2769 - val_acc: 0.4093\n",
      "Epoch 2/200\n",
      "8s - loss: 3.1199 - acc: 0.4082 - val_loss: 2.9796 - val_acc: 0.4093\n",
      "Epoch 3/200\n",
      "8s - loss: 2.8796 - acc: 0.4082 - val_loss: 2.7922 - val_acc: 0.4093\n",
      "Epoch 4/200\n",
      "8s - loss: 2.7446 - acc: 0.4306 - val_loss: 2.6981 - val_acc: 0.4445\n",
      "Epoch 5/200\n",
      "8s - loss: 2.6675 - acc: 0.4543 - val_loss: 2.6313 - val_acc: 0.4616\n",
      "Epoch 6/200\n",
      "8s - loss: 2.6083 - acc: 0.4653 - val_loss: 2.5823 - val_acc: 0.4645\n",
      "Epoch 7/200\n",
      "8s - loss: 2.5532 - acc: 0.4743 - val_loss: 2.5177 - val_acc: 0.4793\n",
      "Epoch 8/200\n",
      "8s - loss: 2.4848 - acc: 0.4805 - val_loss: 2.4479 - val_acc: 0.4920\n",
      "Epoch 9/200\n",
      "8s - loss: 2.4195 - acc: 0.4916 - val_loss: 2.3857 - val_acc: 0.4926\n",
      "Epoch 10/200\n",
      "8s - loss: 2.3575 - acc: 0.4922 - val_loss: 2.3193 - val_acc: 0.4937\n",
      "Epoch 11/200\n",
      "8s - loss: 2.2791 - acc: 0.4946 - val_loss: 2.2341 - val_acc: 0.4990\n",
      "Epoch 12/200\n",
      "8s - loss: 2.2032 - acc: 0.4988 - val_loss: 2.1717 - val_acc: 0.5002\n",
      "Epoch 13/200\n",
      "8s - loss: 2.1512 - acc: 0.5030 - val_loss: 2.1293 - val_acc: 0.5075\n",
      "Epoch 14/200\n",
      "8s - loss: 2.1138 - acc: 0.5040 - val_loss: 2.0993 - val_acc: 0.5068\n",
      "Epoch 15/200\n",
      "8s - loss: 2.0844 - acc: 0.5039 - val_loss: 2.0694 - val_acc: 0.5028\n",
      "Epoch 16/200\n",
      "8s - loss: 2.0595 - acc: 0.5036 - val_loss: 2.0463 - val_acc: 0.5036\n",
      "Epoch 17/200\n",
      "8s - loss: 2.0374 - acc: 0.5050 - val_loss: 2.0261 - val_acc: 0.5063\n",
      "Epoch 18/200\n",
      "8s - loss: 2.0167 - acc: 0.5081 - val_loss: 2.0063 - val_acc: 0.5117\n",
      "Epoch 19/200\n",
      "8s - loss: 1.9974 - acc: 0.5120 - val_loss: 1.9867 - val_acc: 0.5158\n",
      "Epoch 20/200\n",
      "8s - loss: 1.9780 - acc: 0.5158 - val_loss: 1.9661 - val_acc: 0.5184\n",
      "Epoch 21/200\n",
      "8s - loss: 1.9596 - acc: 0.5188 - val_loss: 1.9506 - val_acc: 0.5191\n",
      "Epoch 22/200\n",
      "8s - loss: 1.9440 - acc: 0.5203 - val_loss: 1.9341 - val_acc: 0.5217\n",
      "Epoch 23/200\n",
      "8s - loss: 1.9292 - acc: 0.5216 - val_loss: 1.9197 - val_acc: 0.5233\n",
      "Epoch 24/200\n",
      "8s - loss: 1.9165 - acc: 0.5223 - val_loss: 1.9083 - val_acc: 0.5229\n",
      "Epoch 25/200\n",
      "8s - loss: 1.9041 - acc: 0.5229 - val_loss: 1.8959 - val_acc: 0.5236\n",
      "Epoch 26/200\n",
      "8s - loss: 1.8942 - acc: 0.5226 - val_loss: 1.8853 - val_acc: 0.5241\n",
      "Epoch 27/200\n",
      "8s - loss: 1.8816 - acc: 0.5232 - val_loss: 1.8727 - val_acc: 0.5244\n",
      "Epoch 28/200\n",
      "8s - loss: 1.8703 - acc: 0.5236 - val_loss: 1.8610 - val_acc: 0.5250\n",
      "Epoch 29/200\n",
      "8s - loss: 1.8592 - acc: 0.5245 - val_loss: 1.8540 - val_acc: 0.5241\n",
      "Epoch 30/200\n",
      "8s - loss: 1.8497 - acc: 0.5253 - val_loss: 1.8405 - val_acc: 0.5277\n",
      "Epoch 31/200\n",
      "8s - loss: 1.8381 - acc: 0.5267 - val_loss: 1.8306 - val_acc: 0.5281\n",
      "Epoch 32/200\n",
      "8s - loss: 1.8274 - acc: 0.5291 - val_loss: 1.8199 - val_acc: 0.5319\n",
      "Epoch 33/200\n",
      "8s - loss: 1.8146 - acc: 0.5330 - val_loss: 1.8047 - val_acc: 0.5340\n",
      "Epoch 34/200\n",
      "8s - loss: 1.8007 - acc: 0.5330 - val_loss: 1.7909 - val_acc: 0.5339\n",
      "Epoch 35/200\n",
      "8s - loss: 1.7832 - acc: 0.5357 - val_loss: 1.7696 - val_acc: 0.5389\n",
      "Epoch 36/200\n",
      "8s - loss: 1.7609 - acc: 0.5461 - val_loss: 1.7461 - val_acc: 0.5510\n",
      "Epoch 37/200\n",
      "8s - loss: 1.7358 - acc: 0.5524 - val_loss: 1.7197 - val_acc: 0.5556\n",
      "Epoch 38/200\n",
      "8s - loss: 1.7124 - acc: 0.5558 - val_loss: 1.6981 - val_acc: 0.5576\n",
      "Epoch 39/200\n",
      "8s - loss: 1.6927 - acc: 0.5578 - val_loss: 1.6799 - val_acc: 0.5604\n",
      "Epoch 40/200\n",
      "8s - loss: 1.6749 - acc: 0.5598 - val_loss: 1.6649 - val_acc: 0.5615\n",
      "Epoch 41/200\n",
      "8s - loss: 1.6611 - acc: 0.5613 - val_loss: 1.6522 - val_acc: 0.5647\n",
      "Epoch 42/200\n",
      "8s - loss: 1.6473 - acc: 0.5631 - val_loss: 1.6399 - val_acc: 0.5659\n",
      "Epoch 43/200\n",
      "8s - loss: 1.6354 - acc: 0.5647 - val_loss: 1.6268 - val_acc: 0.5666\n",
      "Epoch 44/200\n",
      "8s - loss: 1.6236 - acc: 0.5675 - val_loss: 1.6157 - val_acc: 0.5719\n",
      "Epoch 45/200\n",
      "8s - loss: 1.6140 - acc: 0.5698 - val_loss: 1.6075 - val_acc: 0.5676\n",
      "Epoch 46/200\n",
      "8s - loss: 1.6045 - acc: 0.5701 - val_loss: 1.5980 - val_acc: 0.5741\n",
      "Epoch 47/200\n",
      "8s - loss: 1.5956 - acc: 0.5725 - val_loss: 1.5891 - val_acc: 0.5752\n",
      "Epoch 48/200\n",
      "8s - loss: 1.5878 - acc: 0.5751 - val_loss: 1.5808 - val_acc: 0.5779\n",
      "Epoch 49/200\n",
      "8s - loss: 1.5803 - acc: 0.5768 - val_loss: 1.5749 - val_acc: 0.5764\n",
      "Epoch 50/200\n",
      "8s - loss: 1.5736 - acc: 0.5778 - val_loss: 1.5668 - val_acc: 0.5810\n",
      "Epoch 51/200\n",
      "8s - loss: 1.5631 - acc: 0.5804 - val_loss: 1.5572 - val_acc: 0.5827\n",
      "Epoch 52/200\n",
      "8s - loss: 1.5556 - acc: 0.5814 - val_loss: 1.5493 - val_acc: 0.5827\n",
      "Epoch 53/200\n",
      "8s - loss: 1.5486 - acc: 0.5824 - val_loss: 1.5428 - val_acc: 0.5851\n",
      "Epoch 54/200\n",
      "8s - loss: 1.5416 - acc: 0.5833 - val_loss: 1.5357 - val_acc: 0.5850\n",
      "Epoch 55/200\n",
      "8s - loss: 1.5353 - acc: 0.5838 - val_loss: 1.5295 - val_acc: 0.5847\n",
      "Epoch 56/200\n",
      "8s - loss: 1.5286 - acc: 0.5850 - val_loss: 1.5233 - val_acc: 0.5877\n",
      "Epoch 57/200\n",
      "8s - loss: 1.5229 - acc: 0.5861 - val_loss: 1.5205 - val_acc: 0.5872\n",
      "Epoch 58/200\n",
      "8s - loss: 1.5169 - acc: 0.5869 - val_loss: 1.5135 - val_acc: 0.5863\n",
      "Epoch 59/200\n",
      "8s - loss: 1.5107 - acc: 0.5877 - val_loss: 1.5069 - val_acc: 0.5894\n",
      "Epoch 60/200\n",
      "8s - loss: 1.5054 - acc: 0.5882 - val_loss: 1.5060 - val_acc: 0.5880\n",
      "Epoch 61/200\n",
      "8s - loss: 1.4997 - acc: 0.5888 - val_loss: 1.4952 - val_acc: 0.5894\n",
      "Epoch 62/200\n",
      "8s - loss: 1.4938 - acc: 0.5898 - val_loss: 1.4904 - val_acc: 0.5901\n",
      "Epoch 63/200\n",
      "8s - loss: 1.4902 - acc: 0.5918 - val_loss: 1.4937 - val_acc: 0.5915\n",
      "Epoch 64/200\n",
      "8s - loss: 1.4841 - acc: 0.5940 - val_loss: 1.4811 - val_acc: 0.5952\n",
      "Epoch 65/200\n",
      "8s - loss: 1.4793 - acc: 0.5950 - val_loss: 1.4748 - val_acc: 0.5986\n",
      "Epoch 66/200\n",
      "8s - loss: 1.4737 - acc: 0.5963 - val_loss: 1.4701 - val_acc: 0.5968\n",
      "Epoch 67/200\n",
      "8s - loss: 1.4686 - acc: 0.5966 - val_loss: 1.4690 - val_acc: 0.5977\n",
      "Epoch 68/200\n",
      "8s - loss: 1.4636 - acc: 0.5976 - val_loss: 1.4643 - val_acc: 0.5967\n",
      "Epoch 69/200\n",
      "8s - loss: 1.4590 - acc: 0.5979 - val_loss: 1.4558 - val_acc: 0.5979\n",
      "Epoch 70/200\n",
      "8s - loss: 1.4539 - acc: 0.5999 - val_loss: 1.4544 - val_acc: 0.6005\n",
      "Epoch 71/200\n",
      "8s - loss: 1.4471 - acc: 0.6013 - val_loss: 1.4426 - val_acc: 0.6013\n",
      "Epoch 72/200\n",
      "8s - loss: 1.4416 - acc: 0.6021 - val_loss: 1.4400 - val_acc: 0.6008\n",
      "Epoch 73/200\n",
      "8s - loss: 1.4371 - acc: 0.6024 - val_loss: 1.4311 - val_acc: 0.6036\n",
      "Epoch 74/200\n",
      "8s - loss: 1.4317 - acc: 0.6032 - val_loss: 1.4273 - val_acc: 0.6047\n",
      "Epoch 75/200\n",
      "8s - loss: 1.4262 - acc: 0.6041 - val_loss: 1.4247 - val_acc: 0.6034\n",
      "Epoch 76/200\n",
      "8s - loss: 1.4223 - acc: 0.6045 - val_loss: 1.4183 - val_acc: 0.6053\n",
      "Epoch 77/200\n",
      "8s - loss: 1.4181 - acc: 0.6053 - val_loss: 1.4163 - val_acc: 0.6054\n",
      "Epoch 78/200\n",
      "8s - loss: 1.4145 - acc: 0.6060 - val_loss: 1.4128 - val_acc: 0.6053\n",
      "Epoch 79/200\n",
      "8s - loss: 1.4118 - acc: 0.6061 - val_loss: 1.4082 - val_acc: 0.6072\n",
      "Epoch 80/200\n",
      "8s - loss: 1.4074 - acc: 0.6072 - val_loss: 1.4048 - val_acc: 0.6088\n",
      "Epoch 81/200\n",
      "8s - loss: 1.4054 - acc: 0.6074 - val_loss: 1.4064 - val_acc: 0.6077\n",
      "Epoch 82/200\n",
      "8s - loss: 1.4017 - acc: 0.6080 - val_loss: 1.3993 - val_acc: 0.6091\n",
      "Epoch 83/200\n",
      "8s - loss: 1.3989 - acc: 0.6089 - val_loss: 1.3968 - val_acc: 0.6099\n",
      "Epoch 84/200\n",
      "8s - loss: 1.3965 - acc: 0.6096 - val_loss: 1.3931 - val_acc: 0.6110\n",
      "Epoch 85/200\n",
      "8s - loss: 1.3932 - acc: 0.6109 - val_loss: 1.3948 - val_acc: 0.6103\n",
      "Epoch 86/200\n",
      "8s - loss: 1.3897 - acc: 0.6120 - val_loss: 1.3861 - val_acc: 0.6139\n",
      "Epoch 87/200\n",
      "8s - loss: 1.3872 - acc: 0.6130 - val_loss: 1.3871 - val_acc: 0.6136\n",
      "Epoch 88/200\n",
      "8s - loss: 1.3844 - acc: 0.6140 - val_loss: 1.3864 - val_acc: 0.6130\n",
      "Epoch 89/200\n",
      "8s - loss: 1.3824 - acc: 0.6147 - val_loss: 1.3797 - val_acc: 0.6142\n",
      "Epoch 90/200\n",
      "8s - loss: 1.3808 - acc: 0.6155 - val_loss: 1.3769 - val_acc: 0.6152\n",
      "Epoch 91/200\n",
      "8s - loss: 1.3780 - acc: 0.6160 - val_loss: 1.3767 - val_acc: 0.6169\n",
      "Epoch 92/200\n",
      "8s - loss: 1.3763 - acc: 0.6167 - val_loss: 1.3759 - val_acc: 0.6170\n",
      "Epoch 93/200\n",
      "8s - loss: 1.3744 - acc: 0.6172 - val_loss: 1.3721 - val_acc: 0.6181\n",
      "Epoch 94/200\n",
      "8s - loss: 1.3730 - acc: 0.6181 - val_loss: 1.3761 - val_acc: 0.6175\n",
      "Epoch 95/200\n",
      "8s - loss: 1.3714 - acc: 0.6182 - val_loss: 1.3666 - val_acc: 0.6203\n",
      "Epoch 96/200\n",
      "8s - loss: 1.3684 - acc: 0.6192 - val_loss: 1.3656 - val_acc: 0.6194\n",
      "Epoch 97/200\n",
      "8s - loss: 1.3664 - acc: 0.6195 - val_loss: 1.3639 - val_acc: 0.6200\n",
      "Epoch 98/200\n",
      "8s - loss: 1.3654 - acc: 0.6199 - val_loss: 1.3625 - val_acc: 0.6210\n",
      "Epoch 99/200\n",
      "8s - loss: 1.3627 - acc: 0.6208 - val_loss: 1.3593 - val_acc: 0.6213\n",
      "Epoch 100/200\n",
      "8s - loss: 1.3616 - acc: 0.6206 - val_loss: 1.3710 - val_acc: 0.6181\n",
      "Epoch 101/200\n",
      "8s - loss: 1.3614 - acc: 0.6210 - val_loss: 1.3566 - val_acc: 0.6212\n",
      "Epoch 102/200\n",
      "8s - loss: 1.3572 - acc: 0.6218 - val_loss: 1.3545 - val_acc: 0.6228\n",
      "Epoch 103/200\n",
      "8s - loss: 1.3560 - acc: 0.6219 - val_loss: 1.3540 - val_acc: 0.6229\n",
      "Epoch 104/200\n",
      "8s - loss: 1.3557 - acc: 0.6219 - val_loss: 1.3528 - val_acc: 0.6233\n",
      "Epoch 105/200\n",
      "8s - loss: 1.3536 - acc: 0.6226 - val_loss: 1.3579 - val_acc: 0.6207\n",
      "Epoch 106/200\n",
      "8s - loss: 1.3508 - acc: 0.6232 - val_loss: 1.3488 - val_acc: 0.6233\n",
      "Epoch 107/200\n",
      "8s - loss: 1.3495 - acc: 0.6233 - val_loss: 1.3465 - val_acc: 0.6232\n",
      "Epoch 108/200\n",
      "8s - loss: 1.3498 - acc: 0.6232 - val_loss: 1.3484 - val_acc: 0.6238\n",
      "Epoch 109/200\n",
      "8s - loss: 1.3466 - acc: 0.6237 - val_loss: 1.3440 - val_acc: 0.6242\n",
      "Epoch 110/200\n",
      "8s - loss: 1.3458 - acc: 0.6239 - val_loss: 1.3438 - val_acc: 0.6241\n",
      "Epoch 111/200\n",
      "8s - loss: 1.3430 - acc: 0.6244 - val_loss: 1.3404 - val_acc: 0.6249\n",
      "Epoch 112/200\n",
      "8s - loss: 1.3425 - acc: 0.6242 - val_loss: 1.3418 - val_acc: 0.6247\n",
      "Epoch 113/200\n",
      "8s - loss: 1.3412 - acc: 0.6243 - val_loss: 1.3379 - val_acc: 0.6246\n",
      "Epoch 114/200\n",
      "8s - loss: 1.3395 - acc: 0.6246 - val_loss: 1.3390 - val_acc: 0.6256\n",
      "Epoch 115/200\n",
      "8s - loss: 1.3382 - acc: 0.6247 - val_loss: 1.3360 - val_acc: 0.6253\n",
      "Epoch 116/200\n",
      "8s - loss: 1.3364 - acc: 0.6250 - val_loss: 1.3460 - val_acc: 0.6231\n",
      "Epoch 117/200\n",
      "8s - loss: 1.3365 - acc: 0.6249 - val_loss: 1.3324 - val_acc: 0.6258\n",
      "Epoch 118/200\n",
      "8s - loss: 1.3336 - acc: 0.6254 - val_loss: 1.3311 - val_acc: 0.6264\n",
      "Epoch 119/200\n",
      "8s - loss: 1.3327 - acc: 0.6254 - val_loss: 1.3310 - val_acc: 0.6264\n",
      "Epoch 120/200\n",
      "8s - loss: 1.3314 - acc: 0.6257 - val_loss: 1.3289 - val_acc: 0.6263\n",
      "Epoch 121/200\n",
      "8s - loss: 1.3316 - acc: 0.6254 - val_loss: 1.3263 - val_acc: 0.6268\n",
      "Epoch 122/200\n",
      "8s - loss: 1.3276 - acc: 0.6261 - val_loss: 1.3267 - val_acc: 0.6269\n",
      "Epoch 123/200\n",
      "8s - loss: 1.3270 - acc: 0.6262 - val_loss: 1.3285 - val_acc: 0.6268\n",
      "Epoch 124/200\n",
      "8s - loss: 1.3257 - acc: 0.6265 - val_loss: 1.3239 - val_acc: 0.6265\n",
      "Epoch 125/200\n",
      "8s - loss: 1.3239 - acc: 0.6267 - val_loss: 1.3233 - val_acc: 0.6275\n",
      "Epoch 126/200\n",
      "8s - loss: 1.3247 - acc: 0.6267 - val_loss: 1.3195 - val_acc: 0.6281\n",
      "Epoch 127/200\n",
      "8s - loss: 1.3224 - acc: 0.6272 - val_loss: 1.3198 - val_acc: 0.6275\n",
      "Epoch 128/200\n",
      "8s - loss: 1.3216 - acc: 0.6272 - val_loss: 1.3180 - val_acc: 0.6285\n",
      "Epoch 129/200\n",
      "8s - loss: 1.3224 - acc: 0.6269 - val_loss: 1.3208 - val_acc: 0.6276\n",
      "Epoch 130/200\n",
      "8s - loss: 1.3193 - acc: 0.6279 - val_loss: 1.3197 - val_acc: 0.6270\n",
      "Epoch 131/200\n",
      "8s - loss: 1.3198 - acc: 0.6273 - val_loss: 1.3176 - val_acc: 0.6287\n",
      "Epoch 132/200\n",
      "8s - loss: 1.3161 - acc: 0.6282 - val_loss: 1.3208 - val_acc: 0.6266\n",
      "Epoch 133/200\n",
      "8s - loss: 1.3161 - acc: 0.6283 - val_loss: 1.3132 - val_acc: 0.6290\n",
      "Epoch 134/200\n",
      "8s - loss: 1.3154 - acc: 0.6283 - val_loss: 1.3130 - val_acc: 0.6290\n",
      "Epoch 135/200\n",
      "8s - loss: 1.3133 - acc: 0.6288 - val_loss: 1.3130 - val_acc: 0.6286\n",
      "Epoch 136/200\n",
      "8s - loss: 1.3124 - acc: 0.6288 - val_loss: 1.3101 - val_acc: 0.6300\n",
      "Epoch 137/200\n",
      "8s - loss: 1.3116 - acc: 0.6290 - val_loss: 1.3086 - val_acc: 0.6297\n",
      "Epoch 138/200\n",
      "8s - loss: 1.3114 - acc: 0.6287 - val_loss: 1.3142 - val_acc: 0.6273\n",
      "Epoch 139/200\n",
      "8s - loss: 1.3101 - acc: 0.6292 - val_loss: 1.3092 - val_acc: 0.6300\n",
      "Epoch 140/200\n",
      "8s - loss: 1.3080 - acc: 0.6293 - val_loss: 1.3083 - val_acc: 0.6302\n",
      "Epoch 141/200\n",
      "8s - loss: 1.3069 - acc: 0.6298 - val_loss: 1.3044 - val_acc: 0.6291\n",
      "Epoch 142/200\n",
      "8s - loss: 1.3067 - acc: 0.6294 - val_loss: 1.3026 - val_acc: 0.6305\n",
      "Epoch 143/200\n",
      "8s - loss: 1.3058 - acc: 0.6299 - val_loss: 1.3025 - val_acc: 0.6293\n",
      "Epoch 144/200\n",
      "8s - loss: 1.3088 - acc: 0.6288 - val_loss: 1.3062 - val_acc: 0.6298\n",
      "Epoch 145/200\n",
      "8s - loss: 1.3056 - acc: 0.6295 - val_loss: 1.3080 - val_acc: 0.6282\n",
      "Epoch 146/200\n",
      "8s - loss: 1.3024 - acc: 0.6305 - val_loss: 1.3030 - val_acc: 0.6301\n",
      "Epoch 147/200\n",
      "8s - loss: 1.3034 - acc: 0.6301 - val_loss: 1.3109 - val_acc: 0.6287\n",
      "Epoch 148/200\n",
      "8s - loss: 1.3027 - acc: 0.6299 - val_loss: 1.2980 - val_acc: 0.6317\n",
      "Epoch 149/200\n",
      "8s - loss: 1.3020 - acc: 0.6301 - val_loss: 1.3007 - val_acc: 0.6312\n",
      "Epoch 150/200\n",
      "8s - loss: 1.3002 - acc: 0.6305 - val_loss: 1.2977 - val_acc: 0.6322\n",
      "Epoch 151/200\n",
      "8s - loss: 1.2997 - acc: 0.6307 - val_loss: 1.2979 - val_acc: 0.6320\n",
      "Epoch 152/200\n",
      "8s - loss: 1.2974 - acc: 0.6313 - val_loss: 1.2943 - val_acc: 0.6323\n",
      "Epoch 153/200\n",
      "8s - loss: 1.2972 - acc: 0.6312 - val_loss: 1.2949 - val_acc: 0.6324\n",
      "Epoch 154/200\n",
      "8s - loss: 1.2969 - acc: 0.6312 - val_loss: 1.2955 - val_acc: 0.6330\n",
      "Epoch 155/200\n",
      "8s - loss: 1.2948 - acc: 0.6317 - val_loss: 1.2994 - val_acc: 0.6313\n",
      "Epoch 156/200\n",
      "8s - loss: 1.2954 - acc: 0.6316 - val_loss: 1.2934 - val_acc: 0.6329\n",
      "Epoch 157/200\n",
      "8s - loss: 1.2933 - acc: 0.6320 - val_loss: 1.2935 - val_acc: 0.6321\n",
      "Epoch 158/200\n",
      "8s - loss: 1.2908 - acc: 0.6326 - val_loss: 1.2951 - val_acc: 0.6325\n",
      "Epoch 159/200\n",
      "8s - loss: 1.2922 - acc: 0.6322 - val_loss: 1.2954 - val_acc: 0.6333\n",
      "Epoch 160/200\n",
      "8s - loss: 1.2888 - acc: 0.6330 - val_loss: 1.2856 - val_acc: 0.6338\n",
      "Epoch 161/200\n",
      "8s - loss: 1.2887 - acc: 0.6330 - val_loss: 1.2895 - val_acc: 0.6312\n",
      "Epoch 162/200\n",
      "8s - loss: 1.2910 - acc: 0.6319 - val_loss: 1.2839 - val_acc: 0.6341\n",
      "Epoch 163/200\n",
      "8s - loss: 1.2853 - acc: 0.6336 - val_loss: 1.3009 - val_acc: 0.6280\n",
      "Epoch 164/200\n",
      "8s - loss: 1.2915 - acc: 0.6318 - val_loss: 1.2826 - val_acc: 0.6347\n",
      "Epoch 165/200\n",
      "8s - loss: 1.2878 - acc: 0.6327 - val_loss: 1.2823 - val_acc: 0.6329\n",
      "Epoch 166/200\n",
      "8s - loss: 1.2845 - acc: 0.6334 - val_loss: 1.2796 - val_acc: 0.6346\n",
      "Epoch 167/200\n",
      "8s - loss: 1.2843 - acc: 0.6335 - val_loss: 1.2899 - val_acc: 0.6318\n",
      "Epoch 168/200\n",
      "8s - loss: 1.2809 - acc: 0.6343 - val_loss: 1.2820 - val_acc: 0.6353\n",
      "Epoch 169/200\n",
      "8s - loss: 1.2844 - acc: 0.6332 - val_loss: 1.2784 - val_acc: 0.6349\n",
      "Epoch 170/200\n",
      "8s - loss: 1.2797 - acc: 0.6345 - val_loss: 1.2755 - val_acc: 0.6357\n",
      "Epoch 171/200\n",
      "8s - loss: 1.2785 - acc: 0.6345 - val_loss: 1.2767 - val_acc: 0.6347\n",
      "Epoch 172/200\n",
      "8s - loss: 1.2798 - acc: 0.6339 - val_loss: 1.2752 - val_acc: 0.6356\n",
      "Epoch 173/200\n",
      "8s - loss: 1.2774 - acc: 0.6349 - val_loss: 1.2737 - val_acc: 0.6353\n",
      "Epoch 174/200\n",
      "8s - loss: 1.2777 - acc: 0.6344 - val_loss: 1.2715 - val_acc: 0.6361\n",
      "Epoch 175/200\n",
      "8s - loss: 1.2759 - acc: 0.6346 - val_loss: 1.2775 - val_acc: 0.6334\n",
      "Epoch 176/200\n",
      "8s - loss: 1.2745 - acc: 0.6352 - val_loss: 1.2685 - val_acc: 0.6373\n",
      "Epoch 177/200\n",
      "8s - loss: 1.2731 - acc: 0.6357 - val_loss: 1.2747 - val_acc: 0.6358\n",
      "Epoch 178/200\n",
      "8s - loss: 1.2758 - acc: 0.6348 - val_loss: 1.2728 - val_acc: 0.6354\n",
      "Epoch 179/200\n",
      "8s - loss: 1.2706 - acc: 0.6362 - val_loss: 1.2701 - val_acc: 0.6361\n",
      "Epoch 180/200\n",
      "8s - loss: 1.2752 - acc: 0.6345 - val_loss: 1.2664 - val_acc: 0.6377\n",
      "Epoch 181/200\n",
      "8s - loss: 1.2678 - acc: 0.6365 - val_loss: 1.2694 - val_acc: 0.6352\n",
      "Epoch 182/200\n",
      "8s - loss: 1.2679 - acc: 0.6365 - val_loss: 1.2658 - val_acc: 0.6374\n",
      "Epoch 183/200\n",
      "8s - loss: 1.2685 - acc: 0.6363 - val_loss: 1.2704 - val_acc: 0.6343\n",
      "Epoch 184/200\n",
      "8s - loss: 1.2651 - acc: 0.6370 - val_loss: 1.2600 - val_acc: 0.6367\n",
      "Epoch 185/200\n",
      "8s - loss: 1.2688 - acc: 0.6357 - val_loss: 1.2655 - val_acc: 0.6359\n",
      "Epoch 186/200\n",
      "8s - loss: 1.2678 - acc: 0.6358 - val_loss: 1.2597 - val_acc: 0.6373\n",
      "Epoch 187/200\n",
      "8s - loss: 1.2628 - acc: 0.6370 - val_loss: 1.2673 - val_acc: 0.6369\n",
      "Epoch 188/200\n",
      "8s - loss: 1.2624 - acc: 0.6371 - val_loss: 1.2610 - val_acc: 0.6384\n",
      "Epoch 189/200\n",
      "8s - loss: 1.2603 - acc: 0.6380 - val_loss: 1.2555 - val_acc: 0.6396\n",
      "Epoch 190/200\n",
      "8s - loss: 1.2578 - acc: 0.6384 - val_loss: 1.2779 - val_acc: 0.6331\n",
      "Epoch 191/200\n",
      "8s - loss: 1.2637 - acc: 0.6368 - val_loss: 1.2522 - val_acc: 0.6400\n",
      "Epoch 192/200\n",
      "8s - loss: 1.2547 - acc: 0.6392 - val_loss: 1.2510 - val_acc: 0.6408\n",
      "Epoch 193/200\n",
      "8s - loss: 1.2632 - acc: 0.6368 - val_loss: 1.2657 - val_acc: 0.6365\n",
      "Epoch 194/200\n",
      "8s - loss: 1.2599 - acc: 0.6372 - val_loss: 1.2523 - val_acc: 0.6393\n",
      "Epoch 195/200\n",
      "8s - loss: 1.2581 - acc: 0.6382 - val_loss: 1.2707 - val_acc: 0.6343\n",
      "Epoch 196/200\n",
      "8s - loss: 1.2549 - acc: 0.6388 - val_loss: 1.2686 - val_acc: 0.6367\n",
      "Epoch 197/200\n",
      "8s - loss: 1.2537 - acc: 0.6394 - val_loss: 1.2520 - val_acc: 0.6391\n",
      "Epoch 198/200\n",
      "8s - loss: 1.2518 - acc: 0.6400 - val_loss: 1.2510 - val_acc: 0.6397\n",
      "Epoch 199/200\n",
      "8s - loss: 1.2540 - acc: 0.6389 - val_loss: 1.2502 - val_acc: 0.6398\n",
      "Epoch 200/200\n",
      "8s - loss: 1.2512 - acc: 0.6397 - val_loss: 1.2523 - val_acc: 0.6396\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIhIJyUFERHp9P8BPrHqkUm55j4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2512a4886d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "new jersey est parfois agrã©able en mois et il est il en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import RepeatVector\n",
    "history = History()                 #need this in order to do my vsualizations\n",
    "\n",
    "def encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train an encoder-decoder model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # OPTIONAL: Implement -- Done!\n",
    "    \"\"\"\n",
    "    RepeatVector -- Repeats the input n times i.e. output_sequence_length times\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(GRU(output_sequence_length, input_shape=input_shape[1:], return_sequences=False))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    model.add(GRU(output_sequence_length, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size)))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tests.test_encdec_model(encdec_model)\n",
    "\n",
    "\n",
    "# OPTIONAL: Train and Print prediction(s) -- Done!\n",
    "#train\n",
    "encdec_rnn_model = encdec_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1)\n",
    "encdec_rnn_model.summary()\n",
    "encdec_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=200, validation_split=0.2, verbose=2, callbacks=[history])\n",
    "\n",
    "# Add visualization\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('encode-decode model loss 5 epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train loss', 'validation loss'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#Print\n",
    "print(english_sentences[0])\n",
    "print(logits_to_text(encdec_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![RNN](encode-decode model loss 5 epochs.png)![RNN](encode-decode model loss 200 epochs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model 5: Custom (IMPLEMENTATION)\n",
    "Use everything you learned from the previous models to create a model that incorporates embedding and a bidirectional rnn into one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Loaded\n"
     ]
    }
   ],
   "source": [
    "history = History()     \n",
    "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement -- Done!\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(english_vocab_size, french_vocab_size, input_length=input_shape[1], mask_zero=True))\n",
    "    model.add(Bidirectional(GRU(64, return_sequences=False)))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    model.add(GRU(64, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size)))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tests.test_model_final(model_final)\n",
    "\n",
    "\n",
    "print('Final Model Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prediction (IMPLEMENTATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 15, 346)           69200     \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 128)               157824    \n",
      "_________________________________________________________________\n",
      "repeat_vector_4 (RepeatVecto (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "gru_40 (GRU)                 (None, 21, 64)            37056     \n",
      "_________________________________________________________________\n",
      "time_distributed_36 (TimeDis (None, 21, 346)           22490     \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 21, 346)           0         \n",
      "=================================================================\n",
      "Total params: 286,570\n",
      "Trainable params: 286,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/100\n",
      "21s - loss: 3.3659 - acc: 0.4180 - val_loss: 2.6809 - val_acc: 0.4480\n",
      "Epoch 2/100\n",
      "18s - loss: 2.4880 - acc: 0.4702 - val_loss: 2.2882 - val_acc: 0.4870\n",
      "Epoch 3/100\n",
      "18s - loss: 2.1780 - acc: 0.4885 - val_loss: 2.0876 - val_acc: 0.4964\n",
      "Epoch 4/100\n",
      "18s - loss: 2.0357 - acc: 0.5041 - val_loss: 1.9865 - val_acc: 0.5199\n",
      "Epoch 5/100\n",
      "18s - loss: 1.9473 - acc: 0.5197 - val_loss: 1.9076 - val_acc: 0.5253\n",
      "Epoch 6/100\n",
      "17s - loss: 1.8800 - acc: 0.5238 - val_loss: 1.8523 - val_acc: 0.5247\n",
      "Epoch 7/100\n",
      "18s - loss: 1.8207 - acc: 0.5311 - val_loss: 1.7882 - val_acc: 0.5366\n",
      "Epoch 8/100\n",
      "18s - loss: 1.7608 - acc: 0.5379 - val_loss: 1.7308 - val_acc: 0.5402\n",
      "Epoch 9/100\n",
      "18s - loss: 1.7033 - acc: 0.5498 - val_loss: 1.6723 - val_acc: 0.5602\n",
      "Epoch 10/100\n",
      "18s - loss: 1.6462 - acc: 0.5711 - val_loss: 1.6175 - val_acc: 0.5795\n",
      "Epoch 11/100\n",
      "18s - loss: 1.5898 - acc: 0.5897 - val_loss: 1.5566 - val_acc: 0.5986\n",
      "Epoch 12/100\n",
      "18s - loss: 1.5284 - acc: 0.6053 - val_loss: 1.4947 - val_acc: 0.6126\n",
      "Epoch 13/100\n",
      "18s - loss: 1.4677 - acc: 0.6213 - val_loss: 1.4364 - val_acc: 0.6282\n",
      "Epoch 14/100\n",
      "18s - loss: 1.4121 - acc: 0.6318 - val_loss: 1.3861 - val_acc: 0.6352\n",
      "Epoch 15/100\n",
      "18s - loss: 1.3614 - acc: 0.6387 - val_loss: 1.3335 - val_acc: 0.6446\n",
      "Epoch 16/100\n",
      "18s - loss: 1.3140 - acc: 0.6477 - val_loss: 1.2916 - val_acc: 0.6524\n",
      "Epoch 17/100\n",
      "18s - loss: 1.2646 - acc: 0.6569 - val_loss: 1.2383 - val_acc: 0.6631\n",
      "Epoch 18/100\n",
      "18s - loss: 1.2145 - acc: 0.6687 - val_loss: 1.1882 - val_acc: 0.6748\n",
      "Epoch 19/100\n",
      "18s - loss: 1.1665 - acc: 0.6771 - val_loss: 1.1429 - val_acc: 0.6819\n",
      "Epoch 20/100\n",
      "18s - loss: 1.1228 - acc: 0.6836 - val_loss: 1.1010 - val_acc: 0.6895\n",
      "Epoch 21/100\n",
      "18s - loss: 1.0795 - acc: 0.6953 - val_loss: 1.0579 - val_acc: 0.7015\n",
      "Epoch 22/100\n",
      "18s - loss: 1.0369 - acc: 0.7067 - val_loss: 1.0165 - val_acc: 0.7120\n",
      "Epoch 23/100\n",
      "18s - loss: 0.9983 - acc: 0.7156 - val_loss: 0.9790 - val_acc: 0.7206\n",
      "Epoch 24/100\n",
      "18s - loss: 0.9594 - acc: 0.7253 - val_loss: 0.9503 - val_acc: 0.7281\n",
      "Epoch 25/100\n",
      "18s - loss: 0.9224 - acc: 0.7354 - val_loss: 0.9073 - val_acc: 0.7408\n",
      "Epoch 26/100\n",
      "18s - loss: 0.8895 - acc: 0.7481 - val_loss: 0.8736 - val_acc: 0.7542\n",
      "Epoch 27/100\n",
      "18s - loss: 0.8551 - acc: 0.7597 - val_loss: 0.8447 - val_acc: 0.7644\n",
      "Epoch 28/100\n",
      "18s - loss: 0.8247 - acc: 0.7693 - val_loss: 0.8113 - val_acc: 0.7750\n",
      "Epoch 29/100\n",
      "18s - loss: 0.7924 - acc: 0.7788 - val_loss: 0.7850 - val_acc: 0.7806\n",
      "Epoch 30/100\n",
      "18s - loss: 0.7649 - acc: 0.7865 - val_loss: 0.7650 - val_acc: 0.7879\n",
      "Epoch 31/100\n",
      "18s - loss: 0.7417 - acc: 0.7933 - val_loss: 0.7321 - val_acc: 0.7981\n",
      "Epoch 32/100\n",
      "18s - loss: 0.7141 - acc: 0.8027 - val_loss: 0.7105 - val_acc: 0.8027\n",
      "Epoch 33/100\n",
      "18s - loss: 0.6932 - acc: 0.8094 - val_loss: 0.6863 - val_acc: 0.8114\n",
      "Epoch 34/100\n",
      "18s - loss: 0.6706 - acc: 0.8160 - val_loss: 0.6696 - val_acc: 0.8164\n",
      "Epoch 35/100\n",
      "18s - loss: 0.6492 - acc: 0.8221 - val_loss: 0.6491 - val_acc: 0.8233\n",
      "Epoch 36/100\n",
      "18s - loss: 0.6285 - acc: 0.8287 - val_loss: 0.6266 - val_acc: 0.8308\n",
      "Epoch 37/100\n",
      "18s - loss: 0.6138 - acc: 0.8332 - val_loss: 0.6107 - val_acc: 0.8352\n",
      "Epoch 38/100\n",
      "18s - loss: 0.5938 - acc: 0.8395 - val_loss: 0.5916 - val_acc: 0.8397\n",
      "Epoch 39/100\n",
      "18s - loss: 0.5858 - acc: 0.8413 - val_loss: 0.5770 - val_acc: 0.8456\n",
      "Epoch 40/100\n",
      "18s - loss: 0.5601 - acc: 0.8494 - val_loss: 0.5610 - val_acc: 0.8499\n",
      "Epoch 41/100\n",
      "18s - loss: 0.5447 - acc: 0.8532 - val_loss: 0.5474 - val_acc: 0.8539\n",
      "Epoch 42/100\n",
      "18s - loss: 0.5295 - acc: 0.8569 - val_loss: 0.5307 - val_acc: 0.8573\n",
      "Epoch 43/100\n",
      "18s - loss: 0.5161 - acc: 0.8602 - val_loss: 0.5210 - val_acc: 0.8597\n",
      "Epoch 44/100\n",
      "18s - loss: 0.5058 - acc: 0.8622 - val_loss: 0.5122 - val_acc: 0.8611\n",
      "Epoch 45/100\n",
      "18s - loss: 0.4925 - acc: 0.8659 - val_loss: 0.4971 - val_acc: 0.8656\n",
      "Epoch 46/100\n",
      "18s - loss: 0.4802 - acc: 0.8687 - val_loss: 0.4868 - val_acc: 0.8672\n",
      "Epoch 47/100\n",
      "18s - loss: 0.4676 - acc: 0.8720 - val_loss: 0.4719 - val_acc: 0.8717\n",
      "Epoch 48/100\n",
      "18s - loss: 0.4555 - acc: 0.8750 - val_loss: 0.4626 - val_acc: 0.8743\n",
      "Epoch 49/100\n",
      "18s - loss: 0.4463 - acc: 0.8771 - val_loss: 0.4507 - val_acc: 0.8767\n",
      "Epoch 50/100\n",
      "18s - loss: 0.4323 - acc: 0.8807 - val_loss: 0.4378 - val_acc: 0.8807\n",
      "Epoch 51/100\n",
      "18s - loss: 0.4228 - acc: 0.8831 - val_loss: 0.4299 - val_acc: 0.8819\n",
      "Epoch 52/100\n",
      "18s - loss: 0.4126 - acc: 0.8857 - val_loss: 0.4202 - val_acc: 0.8841\n",
      "Epoch 53/100\n",
      "18s - loss: 0.4005 - acc: 0.8885 - val_loss: 0.4076 - val_acc: 0.8878\n",
      "Epoch 54/100\n",
      "18s - loss: 0.3896 - acc: 0.8915 - val_loss: 0.3993 - val_acc: 0.8904\n",
      "Epoch 55/100\n",
      "18s - loss: 0.3783 - acc: 0.8946 - val_loss: 0.3844 - val_acc: 0.8936\n",
      "Epoch 56/100\n",
      "18s - loss: 0.3703 - acc: 0.8971 - val_loss: 0.3841 - val_acc: 0.8944\n",
      "Epoch 57/100\n",
      "18s - loss: 0.3611 - acc: 0.9000 - val_loss: 0.3679 - val_acc: 0.9000\n",
      "Epoch 58/100\n",
      "18s - loss: 0.3512 - acc: 0.9030 - val_loss: 0.3760 - val_acc: 0.8974\n",
      "Epoch 59/100\n",
      "18s - loss: 0.3409 - acc: 0.9061 - val_loss: 0.3537 - val_acc: 0.9039\n",
      "Epoch 60/100\n",
      "18s - loss: 0.3293 - acc: 0.9098 - val_loss: 0.3468 - val_acc: 0.9052\n",
      "Epoch 61/100\n",
      "18s - loss: 0.3228 - acc: 0.9117 - val_loss: 0.3335 - val_acc: 0.9103\n",
      "Epoch 62/100\n",
      "18s - loss: 0.3090 - acc: 0.9162 - val_loss: 0.3213 - val_acc: 0.9137\n",
      "Epoch 63/100\n",
      "18s - loss: 0.3020 - acc: 0.9185 - val_loss: 0.3270 - val_acc: 0.9131\n",
      "Epoch 64/100\n",
      "18s - loss: 0.2943 - acc: 0.9211 - val_loss: 0.3059 - val_acc: 0.9191\n",
      "Epoch 65/100\n",
      "18s - loss: 0.2810 - acc: 0.9250 - val_loss: 0.2998 - val_acc: 0.9204\n",
      "Epoch 66/100\n",
      "18s - loss: 0.2730 - acc: 0.9279 - val_loss: 0.2835 - val_acc: 0.9255\n",
      "Epoch 67/100\n",
      "18s - loss: 0.2649 - acc: 0.9300 - val_loss: 0.2822 - val_acc: 0.9251\n",
      "Epoch 68/100\n",
      "18s - loss: 0.2556 - acc: 0.9330 - val_loss: 0.2707 - val_acc: 0.9298\n",
      "Epoch 69/100\n",
      "18s - loss: 0.2506 - acc: 0.9349 - val_loss: 0.2663 - val_acc: 0.9314\n",
      "Epoch 70/100\n",
      "18s - loss: 0.2409 - acc: 0.9381 - val_loss: 0.2597 - val_acc: 0.9330\n",
      "Epoch 71/100\n",
      "18s - loss: 0.2332 - acc: 0.9405 - val_loss: 0.2601 - val_acc: 0.9343\n",
      "Epoch 72/100\n",
      "18s - loss: 0.2275 - acc: 0.9425 - val_loss: 0.2628 - val_acc: 0.9337\n",
      "Epoch 73/100\n",
      "18s - loss: 0.2199 - acc: 0.9448 - val_loss: 0.2385 - val_acc: 0.9407\n",
      "Epoch 74/100\n",
      "18s - loss: 0.2165 - acc: 0.9460 - val_loss: 0.2336 - val_acc: 0.9422\n",
      "Epoch 75/100\n",
      "18s - loss: 0.2059 - acc: 0.9492 - val_loss: 0.2279 - val_acc: 0.9446\n",
      "Epoch 76/100\n",
      "18s - loss: 0.2018 - acc: 0.9503 - val_loss: 0.2179 - val_acc: 0.9469\n",
      "Epoch 77/100\n",
      "18s - loss: 0.1938 - acc: 0.9528 - val_loss: 0.2158 - val_acc: 0.9483\n",
      "Epoch 78/100\n",
      "18s - loss: 0.1913 - acc: 0.9537 - val_loss: 0.2226 - val_acc: 0.9464\n",
      "Epoch 79/100\n",
      "18s - loss: 0.1875 - acc: 0.9545 - val_loss: 0.2071 - val_acc: 0.9503\n",
      "Epoch 80/100\n",
      "18s - loss: 0.1762 - acc: 0.9577 - val_loss: 0.1984 - val_acc: 0.9529\n",
      "Epoch 81/100\n",
      "18s - loss: 0.1748 - acc: 0.9579 - val_loss: 0.1953 - val_acc: 0.9538\n",
      "Epoch 82/100\n",
      "18s - loss: 0.1653 - acc: 0.9606 - val_loss: 0.1885 - val_acc: 0.9562\n",
      "Epoch 83/100\n",
      "18s - loss: 0.1626 - acc: 0.9614 - val_loss: 0.1879 - val_acc: 0.9553\n",
      "Epoch 84/100\n",
      "18s - loss: 0.1604 - acc: 0.9619 - val_loss: 0.1865 - val_acc: 0.9557\n",
      "Epoch 85/100\n",
      "18s - loss: 0.1537 - acc: 0.9638 - val_loss: 0.1764 - val_acc: 0.9595\n",
      "Epoch 86/100\n",
      "18s - loss: 0.1497 - acc: 0.9648 - val_loss: 0.1780 - val_acc: 0.9586\n",
      "Epoch 87/100\n",
      "18s - loss: 0.1481 - acc: 0.9655 - val_loss: 0.1686 - val_acc: 0.9611\n",
      "Epoch 88/100\n",
      "18s - loss: 0.1415 - acc: 0.9671 - val_loss: 0.1648 - val_acc: 0.9626\n",
      "Epoch 89/100\n",
      "18s - loss: 0.1369 - acc: 0.9684 - val_loss: 0.1613 - val_acc: 0.9633\n",
      "Epoch 90/100\n",
      "18s - loss: 0.1452 - acc: 0.9666 - val_loss: 0.1663 - val_acc: 0.9622\n",
      "Epoch 91/100\n",
      "18s - loss: 0.1370 - acc: 0.9684 - val_loss: 0.1628 - val_acc: 0.9624\n",
      "Epoch 92/100\n",
      "18s - loss: 0.1284 - acc: 0.9705 - val_loss: 0.1568 - val_acc: 0.9635\n",
      "Epoch 93/100\n",
      "18s - loss: 0.1251 - acc: 0.9713 - val_loss: 0.1491 - val_acc: 0.9666\n",
      "Epoch 94/100\n",
      "18s - loss: 0.1196 - acc: 0.9727 - val_loss: 0.1474 - val_acc: 0.9673\n",
      "Epoch 95/100\n",
      "18s - loss: 0.1238 - acc: 0.9716 - val_loss: 0.1628 - val_acc: 0.9628\n",
      "Epoch 96/100\n",
      "18s - loss: 0.1184 - acc: 0.9729 - val_loss: 0.1449 - val_acc: 0.9675\n",
      "Epoch 97/100\n",
      "18s - loss: 0.1111 - acc: 0.9747 - val_loss: 0.1438 - val_acc: 0.9680\n",
      "Epoch 98/100\n",
      "18s - loss: 0.1136 - acc: 0.9742 - val_loss: 0.1412 - val_acc: 0.9682\n",
      "Epoch 99/100\n",
      "18s - loss: 0.1111 - acc: 0.9749 - val_loss: 0.1366 - val_acc: 0.9691\n",
      "Epoch 100/100\n",
      "18s - loss: 0.1046 - acc: 0.9763 - val_loss: 0.1356 - val_acc: 0.9696\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nr4U4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25131abf748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "il a vu un vieux camion jaune <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Il a vu un vieux camion jaune\n",
      "Sample 2:\n",
      "new jersey est parfois calme pendant l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def final_predictions(x, y, x_tk, y_tk):\n",
    "    \"\"\"\n",
    "    Gets predictions using the final model\n",
    "    :param x: Preprocessed English data\n",
    "    :param y: Preprocessed French data\n",
    "    :param x_tk: English tokenizer\n",
    "    :param y_tk: French tokenizer\n",
    "    \"\"\"\n",
    "    # TODO: Train neural network using model_final\n",
    "    model = model_final(x.shape, y.shape[1], len(x_tk.word_index)+1, len(y_tk.word_index)+1)\n",
    "    model.summary()\n",
    "    model.fit(x, y, batch_size=1024, epochs=100, validation_split=0.2, verbose=2, callbacks=[history])\n",
    "\n",
    "    # Add visualization\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('final model loss - 100 epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train loss', 'validation loss'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    ## DON'T EDIT ANYTHING BELOW THIS LINE\n",
    "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "    y_id_to_word[0] = '<PAD>'\n",
    "\n",
    "    sentence = 'he saw a old yellow truck'\n",
    "    sentence = [x_tk.word_index[word] for word in sentence.split()]\n",
    "    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n",
    "    sentences = np.array([sentence[0], x[0]])\n",
    "    predictions = model.predict(sentences, len(sentences))\n",
    "\n",
    "    print('Sample 1:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
    "    print('Il a vu un vieux camion jaune')\n",
    "    print('Sample 2:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in y[0]]))\n",
    "\n",
    "\n",
    "final_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![RNN](final model loss - 5 epochs.png)\n",
    "\n",
    "\n",
    "![RNN](final model loss - 200 epochs.png)\n",
    "This one is overfitting, so we're trying fewer epochs\n",
    "\n",
    "![RNN](final model loss - 30 epochs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Submission\n",
    "When you are ready to submit your project, do the following steps:\n",
    "1. Ensure you pass all points on the [rubric](https://review.udacity.com/#!/rubrics/1004/view).\n",
    "2. Submit the following in a zip file.\n",
    "  - `helper.py`\n",
    "  - `machine_translation.ipynb`\n",
    "  - `machine_translation.html`\n",
    "    - You can export the notebook by navigating to **File -> Download as -> HTML (.html)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
